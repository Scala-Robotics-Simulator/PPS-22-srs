"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[1211],{5100:(e,i,n)=>{n.d(i,{A:()=>o});const o=n.p+"assets/images/policy-behaviors-aa1c2de01e44fa28b89edfc57405e4c2.png"},7593:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"detailed-design/behavior","title":"Behavior","description":"Il modulo Behavior \xe8 il motore decisionale di un\'entit\xe0 dinamica, progettato per operare all\'interno del ciclo di","source":"@site/docs/04-detailed-design/06-behavior.md","sourceDirName":"04-detailed-design","slug":"/detailed-design/behavior","permalink":"/PPS-22-srs/docs/detailed-design/behavior","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"docsSidebar","previous":{"title":"Entity","permalink":"/PPS-22-srs/docs/detailed-design/entity"},"next":{"title":"Action","permalink":"/PPS-22-srs/docs/detailed-design/action"}}');var s=n(4848),r=n(8453);const t={sidebar_position:6},l="Behavior",c={},d=[{value:"Posizionamento nel ciclo di simulazione",id:"posizionamento-nel-ciclo-di-simulazione",level:2},{value:"I/O e contratti",id:"io-e-contratti",level:2},{value:"Pattern Kleisli",id:"pattern-kleisli",level:2},{value:"Astrazione dei comportamenti",id:"astrazione-dei-comportamenti",level:2},{value:"Panoramica tra Policy e Behavior",id:"panoramica-tra-policy-e-behavior",level:2},{value:"DSL di composizione",id:"dsl-di-composizione",level:2},{value:"Politiche predefinite",id:"politiche-predefinite",level:2}];function a(e){const i={a:"a",admonition:"admonition",blockquote:"blockquote",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(i.header,{children:(0,s.jsx)(i.h1,{id:"behavior",children:"Behavior"})}),"\n",(0,s.jsxs)(i.p,{children:["Il modulo ",(0,s.jsx)(i.code,{children:"Behavior"})," \xe8 il ",(0,s.jsx)(i.strong,{children:"motore decisionale"})," di un'entit\xe0 dinamica, progettato per operare all'interno del ciclo di\nsimulazione sense \u2192 plan \u2192 act.\nLa sua responsabilit\xe0 \xe8 ricevere le ",(0,s.jsx)(i.strong,{children:"letture sensoriali"})," (",(0,s.jsx)(i.code,{children:"SensorReadings"}),") e produrre una ",(0,s.jsx)(i.strong,{children:"intenzione"}),", ossia l\u2019\n",(0,s.jsx)(i.code,{children:"Action"})," da eseguire, mantenendo una separazione netta tra la logica decisionale e l'esecuzione fisica dell'azione."]}),"\n",(0,s.jsx)(i.h2,{id:"posizionamento-nel-ciclo-di-simulazione",children:"Posizionamento nel ciclo di simulazione"}),"\n",(0,s.jsxs)(i.p,{children:["Il ",(0,s.jsx)(i.code,{children:"Behavior"})," si inserisce nel ciclo di esecuzione di un'entit\xe0 autonoma:"]}),"\n",(0,s.jsxs)(i.ol,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Sense"}),": l'entit\xe0 acquisisce informazioni sull'ambiente tramite i suoi sensori, producendo un ",(0,s.jsx)(i.code,{children:"SensorReadings"}),";"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Plan"}),": il ",(0,s.jsx)(i.code,{children:"Behavior"})," elabora il contesto e sceglie un\u2019azione appropriata;"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Act"}),": l\u2019azione viene passata agli attuatori, che modificano lo stato dell'entit\xe0 (es. velocit\xe0 delle ruote)."]}),"\n"]}),"\n",(0,s.jsxs)(i.blockquote,{children:["\n",(0,s.jsxs)(i.p,{children:["Nota: il behavior \xe8 ",(0,s.jsx)(i.strong,{children:"stateless"})," (decisione ",(0,s.jsx)(i.em,{children:"cieca"})," sul tick corrente).",(0,s.jsx)(i.br,{}),"\n","Se servisse \u201cmemoria\u201d, bisognerebbe ",(0,s.jsx)(i.strong,{children:"estendere il contesto di input"})," invece di introdurre mutazioni."]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"io-e-contratti",children:"I/O e contratti"}),"\n",(0,s.jsx)(i.p,{children:"Il sistema opera su dati strutturati che incapsulano tutte le informazioni necessarie:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Input"}),": ",(0,s.jsx)(i.code,{children:"BehaviorContext"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.code,{children:"sensorReadings: SensorReadings"}),": le letture sensoriali correnti;"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.code,{children:"rng: RNG"}),": un generatore di numeri pseudo-casuali per comportamenti stocastici e riproducibili."]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Output"})," una ",(0,s.jsx)(i.code,{children:"Decision[F]"})," ossia una funzione totale ",(0,s.jsx)(i.code,{children:"BehaviorContext => (Action[F], RNG)"})," (modellata come\n",(0,s.jsx)(i.em,{children:"Kleisli"}),").","\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.code,{children:"Action[F]"}),": l'azione da eseguire sull'entit\xe0;"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.code,{children:"RNG"}),": lo stato aggiornato del generatore, per garantire che non venga perso il riferimento alla sequenza casuale."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(i.blockquote,{children:["\n",(0,s.jsxs)(i.p,{children:["Nota: un ",(0,s.jsx)(i.code,{children:"Behavior"})," completo deve essere una ",(0,s.jsx)(i.strong,{children:"funzione totale"}),", ossia, dato un qualsiasi ",(0,s.jsx)(i.code,{children:"BehaviorContext"}),"\nvalido, deve ",(0,s.jsx)(i.em,{children:"sempre"})," produrre un'",(0,s.jsx)(i.code,{children:"Action"})," valida."]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"pattern-kleisli",children:"Pattern Kleisli"}),"\n",(0,s.jsxs)(i.p,{children:["I comportamenti sono modellati come funzioni pure da un contesto a una decisione:\n",(0,s.jsx)(i.code,{children:"Decision[F] = Kleisli[Id, BehaviorContext, (Action[F], RNG)]"}),". Questo permette di avere:"]}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"accesso al contesto"}),": ",(0,s.jsx)(i.code,{children:"Kleisli.ask"})," fornisce accesso esplicito al contesto;"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"composizione"}),": con ",(0,s.jsx)(i.code,{children:"Kleisli"})," otteniamo combinatori (",(0,s.jsx)(i.code,{children:"map/flatMap"}),", ",(0,s.jsx)(i.code,{children:"andThen"}),", ",(0,s.jsx)(i.code,{children:"local"}),", ",(0,s.jsx)(i.code,{children:"ask"}),") e un DSL pulito;"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"testabilit\xe0"}),": si testa con ",(0,s.jsx)(i.code,{children:"decision.run(ctx)"})," in modo deterministico;"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"astrazione degli effetti"}),": attualmente utilizziamo ",(0,s.jsx)(i.code,{children:"Id"}),", di conseguenza \xe8 una funziona pura; in futuro pu\xf2\nsfruttare",(0,s.jsx)(i.code,{children:"Either/IO"})," senza cambiare le API del DSL."]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"astrazione-dei-comportamenti",children:"Astrazione dei comportamenti"}),"\n",(0,s.jsx)(i.p,{children:"Il modello si basa su quattro livelli di astrazione:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Condition"}),": ",(0,s.jsx)(i.em,{children:"predicati"})," che valutano lo stato sensoriale (es. soglie/relazioni);"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"PartialBehavior"}),": regole che possono ",(0,s.jsx)(i.em,{children:"proporre"})," ",(0,s.jsx)(i.code,{children:"A"})," (",(0,s.jsx)(i.code,{children:"Some"}),") o defers (",(0,s.jsx)(i.code,{children:"None"}),");"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Behavior"}),": composizione di regole con ",(0,s.jsx)(i.em,{children:"fallback"})," garantito per assicurare totalit\xe0 (sempre un\u2019azione ",(0,s.jsx)(i.code,{children:"A"}),");"]}),"\n",(0,s.jsxs)(i.li,{children:[(0,s.jsx)(i.strong,{children:"Policy"}),": strategie complete e riusabili per casi d'uso specifici (es. evita ostacoli, fototassi, ecc.)."]}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"panoramica-tra-policy-e-behavior",children:"Panoramica tra Policy e Behavior"}),"\n",(0,s.jsxs)(i.p,{children:["Il diagramma seguente sintetizza le relazioni tra ",(0,s.jsx)(i.code,{children:"Policy"}),", i ",(0,s.jsx)(i.code,{children:"Behavior"})," concreti, ",(0,s.jsx)(i.code,{children:"Decision[F]"})," e ",(0,s.jsx)(i.code,{children:"BehaviorContext"}),"."]}),"\n",(0,s.jsx)(i.p,{children:(0,s.jsx)(i.img,{alt:"Policy e Behaviors",src:n(5100).A+"",width:"927",height:"478"})}),"\n",(0,s.jsx)(i.h2,{id:"dsl-di-composizione",children:"DSL di composizione"}),"\n",(0,s.jsx)(i.p,{children:"Il modulo fornisce un linguaggio specifico di dominio per comporre comportamenti in modo dichiarativo:"}),"\n",(0,s.jsx)(i.admonition,{title:"Esempio: evitamento ostacoli con fallback",type:"tip",children:(0,s.jsx)(i.pre,{children:(0,s.jsx)(i.code,{className:"language-scala",children:"((front < 0.30) ==> turnRight[F]) |\n  ((left < 0.25) ==> turnRight[F]) |\n  ((right < 0.25) ==> turnLeft[F])\n    .default(moveForward[F])\n"})})}),"\n",(0,s.jsx)(i.p,{children:"Il DSL supporta:"}),"\n",(0,s.jsxs)(i.ul,{children:["\n",(0,s.jsx)(i.li,{children:"composizione left-biased con priorit\xe0 esplicite;"}),"\n",(0,s.jsx)(i.li,{children:"operatori logici per condizioni complesse;"}),"\n",(0,s.jsx)(i.li,{children:"fallback garantiti per totalit\xe0."}),"\n"]}),"\n",(0,s.jsx)(i.h2,{id:"politiche-predefinite",children:"Politiche predefinite"}),"\n",(0,s.jsx)(i.p,{children:"Sono incluse un insieme di politiche standard che coprono i casi d'uso pi\xf9 comuni:"}),"\n",(0,s.jsxs)(i.table,{children:[(0,s.jsx)(i.thead,{children:(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.th,{children:"Policy"}),(0,s.jsx)(i.th,{children:"Descrizione"}),(0,s.jsx)(i.th,{children:"Caso d'uso"})]})}),(0,s.jsxs)(i.tbody,{children:[(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:(0,s.jsx)(i.strong,{children:"AlwaysForward"})}),(0,s.jsx)(i.td,{children:"Movimento costante in avanti"}),(0,s.jsx)(i.td,{children:"Testing, comportamento base"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:(0,s.jsx)(i.strong,{children:"RandomWalk"})}),(0,s.jsx)(i.td,{children:"Esplorazione stocastica"}),(0,s.jsx)(i.td,{children:"Copertura spaziale"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:(0,s.jsx)(i.strong,{children:"ObstacleAvoidance"})}),(0,s.jsx)(i.td,{children:"Evitamento ostacoli multi-livello"}),(0,s.jsx)(i.td,{children:"Navigazione sicura"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:(0,s.jsx)(i.strong,{children:"Phototaxis"})}),(0,s.jsx)(i.td,{children:"Attrazione verso sorgenti luminose"}),(0,s.jsx)(i.td,{children:"Comportamento biologico"})]}),(0,s.jsxs)(i.tr,{children:[(0,s.jsx)(i.td,{children:(0,s.jsx)(i.strong,{children:"Prioritized"})}),(0,s.jsx)(i.td,{children:"Composizione gerarchica di strategie"}),(0,s.jsx)(i.td,{children:"Comportamenti complessi"})]})]})]}),"\n",(0,s.jsx)(i.admonition,{type:"info",children:(0,s.jsxs)(i.p,{children:["Per i dettagli tecnici di implementazione, consultare la\nsezione ",(0,s.jsx)(i.a,{href:"/PPS-22-srs/docs/implementation/david-cohen/behaviors",children:"implementazione behavior"}),"."]})})]})}function h(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,s.jsx)(i,{...e,children:(0,s.jsx)(a,{...e})}):a(e)}},8453:(e,i,n)=>{n.d(i,{R:()=>t,x:()=>l});var o=n(6540);const s={},r=o.createContext(s);function t(e){const i=o.useContext(r);return o.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function l(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:t(e.components),o.createElement(r.Provider,{value:i},e.children)}}}]);