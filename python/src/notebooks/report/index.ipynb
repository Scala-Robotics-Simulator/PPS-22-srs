{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e813879-0db7-4d71-8dcf-bf003c5e7567",
   "metadata": {},
   "source": [
    "# Obiettivo del progetto\n",
    "\n",
    "L'obiettivo del progetto è estendere il simulatore 2D di robot per dotarlo di comportamenti appresi tramite tecniche di Reinforcement Learning, in particolare Q-learning e Deep Q-learning (DQN).\n",
    "\n",
    "Si è proposto di replicare i seguenti tre comportamenti:\n",
    "- [**phototaxis**](./phototaxis.ipynb): movimento verso una sorgente luminosa;\n",
    "- [**obstacle avoidance**](./obstacle-avoidance.ipynb): movimento con evitamento di ostacoli;\n",
    "- [**exploration**](./exploration.ipynb): massimizzare l’esplorazione dell’ambiente evitando le collisioni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e4d84-9441-4dd7-800f-ed1aa29bf578",
   "metadata": {},
   "source": [
    "# Vincoli\n",
    "\n",
    "I vincoli posti sono:\n",
    "\n",
    "- ambiente: griglie 5×5, 20×20 e 30×30 m;\n",
    "- sensori: 8 prossimità + 8 luce + posizione + orientazione;\n",
    "- luci con raggio 0.2 m e irradiazione 5 m;\n",
    "- ostacoli rettangolari con dimensioni variabili."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f832d-7a24-4a5a-a438-4882d82a6638",
   "metadata": {},
   "source": [
    "# Starting point\n",
    "\n",
    "Siamo partiti da un simulatore in grado di gestire un ambiente composto da N robot aventi un comportamento specificato da configurazione.\n",
    "\n",
    "Nell'ambiente sono presenti entità statiche, come ostacoli e luci e entità dinamiche, come altri robot.\n",
    "I robot sono in grado di percepire le altre entità attraverso i sensori di prossimità e di luce e possono interagire nell''ambiente attraverso gli attuatori tramite comportamenti pre-programmati. Il movimento dei robot segue un movimento differenziale in cui posono essere applicate delle velocità diverse alle due ruote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c2e45a-2232-41e9-986f-0dc31a9fad82",
   "metadata": {},
   "source": [
    "## Configurazione del simulatore (yaml)\n",
    "\n",
    "Per configurare il simulatore si possono utilizzare o file `yaml` oppure la GUI dedicata.\n",
    "\n",
    "L'utilizzo di file `yaml` permette di configurare il simulatore in modo semplice ed efficace. Strutturato come segue:\n",
    "\n",
    "```yaml\n",
    "simulation:\n",
    "  seed: 42\n",
    "environment:\n",
    "  width: 12\n",
    "  height: 10\n",
    "  entities:\n",
    "    - light:\n",
    "        position: [10, 5]\n",
    "        illuminationRadius: 6.0\n",
    "        intensity: 1.0\n",
    "        radius: 0.2\n",
    "        attenuation: 1.0\n",
    "    - obstacle:\n",
    "        position: [6, 5]\n",
    "        orientation: 0.0\n",
    "        width: 2.0\n",
    "        height: 6.0\n",
    "    - robot:\n",
    "        position: [2, 2]\n",
    "        orientation: 45.0\n",
    "        radius: 0.25\n",
    "        speed: 1.0\n",
    "        withProximitySensors: true\n",
    "        withLightSensors: true\n",
    "        behavior: Phototaxis\n",
    "```\n",
    "\n",
    "I file di configurazione si utilizzano durante l'addestramento e la valutazione degli agenti per configurare il simulatore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3cf5cf-c1f8-4c36-9d16-cffbe0f3f145",
   "metadata": {},
   "source": [
    "## Adattamento e modellazione del simulatore\n",
    "\n",
    "L'idea è quella di estendere il simulatore integrando comportamenti basati su tecniche di Q-learning e Deep Q-learning per rendere i robot autonomi.\n",
    "\n",
    "Il simulatore è stato implementato interamente in `Scala`, mentre la parte di Reinforcement Learning è stata implementata in `Python`.\n",
    "\n",
    "### Comunicazione python-simulatore\n",
    "\n",
    "Per il collegamento tra i due linguaggi è stato adottato `gRPC`. Inoltre, è stato definita un’interfaccia di interazione modellata sullo stile di `PettingZoo`, così da mantenere coerenza con le principali librerie Reinforcement Learning multi-agente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e7a04-324d-4939-a0e7-e89e537371bc",
   "metadata": {},
   "source": [
    "### Simulatore lato Scala\n",
    "\n",
    "Di seguito viene fornita una descrizione delle modifiche apportate implementativamente al simulatore.\n",
    "\n",
    "#### Agente\n",
    "\n",
    "Invece che apportare delle modifiche ai robot esistenti si è preferito creare delle entità ad-hoc: gli agenti.\n",
    "Nel simulatore in Scala, l'agente ha le stesse caratteristiche del robot ad eccezione del `Behavior` in cui non associamo più un comportamento programmatico ma adesso è composto da `Reward`, `Termination` e `Truncation`.\n",
    "\n",
    "#### Reward\n",
    "\n",
    "La `Reward` è una funzione associata all'agente che permette di osservare lo stato dell'ambiente (precedente e corrente) e calcolare una ricompensa (bonus o penalità) adeguata al task che deve risolvere.\n",
    "\n",
    "Signature per la reward:\n",
    "\n",
    "```scala\n",
    "def evaluate(prev: BaseState, current: BaseState, entity: Agent, action: Action[?]): Double\n",
    "```\n",
    "\n",
    "PS: nel task di Exploration si è reso necessario aggiungere della memorizzazione # TODO\n",
    "\n",
    "#### Termination\n",
    "\n",
    "La `Termination` permette di concludere in maniera naturale la finestra di addestramento sia come situazione positiva o negativa.\n",
    "Serve ad avere una definition of done al di fuori del numero massimo di step, se definiti.\n",
    "È stata modellata sulla base del task specifico da addestrare come:\n",
    "\n",
    "- collisione dell'agente contro un ostacolo, che sia muro o oggetto.\n",
    "- distanza minima da una fonte di luce.\n",
    "\n",
    "La Signature per la termination viene definita come:\n",
    "\n",
    "```scala\n",
    "def evaluate(prev: BaseState, current: BaseState, entity: Agent, action: Action[?]): Boolean\n",
    "```\n",
    "\n",
    "#### Truncation\n",
    "\n",
    "La `Truncation` è stata realizzata allo stesso modo della `Termination`, ma è stata meno utilizzata rispetto a quella descritta sopra in quanto la gestione del tempo è stata effettuata in python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17638d2a-0811-48ff-aa29-84cb8d7163df",
   "metadata": {},
   "source": [
    "### Lato Python\n",
    "\n",
    "Di seguito sono descritte tutte le aggiunte effettate in `python`.\n",
    "Innanzitutto, è stato creato l'ambiente osservabile dall'agente, la modellazione dell'agente stesso con conseguente implementazione dell'algoritmo di training e valutazione.\n",
    "\n",
    "#### Descrizione dell'ambiente\n",
    "\n",
    "L'ambiente è stato modellato sulla base della libreria `gymnasium`, contiene quindi i metodi:\n",
    "\n",
    "- step:\n",
    "\n",
    "  ```python\n",
    "  def step(self, actions: dict) -> tuple[dict, dict, dict, dict, dict]\n",
    "  ```\n",
    "  \n",
    "  fa svolgere le azioni fornite agli agenti, effettuando un `tick` nel simulatore e restituisce quindi, per ogni agente:\n",
    "\n",
    "  - osservazioni;\n",
    "  - ricompense;\n",
    "  - terminazioni;\n",
    "  - troncamenti;\n",
    "  - informazioni aggiuntive.\n",
    "  \n",
    "- render:\n",
    "  ```python\n",
    "  def render(self, width: int = 800, height: int = 600) -> np.ndarray\n",
    "  ```\n",
    "\n",
    "  ritorna un immagine in formato RGB delle dimensioni indicate, contenente una rappresentazione dello stato del simulatore al momento della chiamata.\n",
    "\n",
    "- reset:\n",
    "\n",
    "  ```python\n",
    "  def reset(self, seed: int = 42) -> tuple[dict, dict]:\n",
    "  ```\n",
    "\n",
    "  riporta il simulatore alla configurazione iniziale e ritorna, per ogni agente:\n",
    "\n",
    "  - osservazioni;\n",
    "  - informazioni aggiuntive.\n",
    "\n",
    "Sono presenti, inoltre, i metodi `init` e `close` utili a inizializzare l'ambiente con la configurazione e chiudere la connessione con `gRPC`. E' \n",
    "\n",
    "#### Agente\n",
    "Q-agent\n",
    "DQ-agent\n",
    "\n",
    "In python invece, dell'agente sono presenti due varianti: il `QAgent` e il `DQAgent`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5798b3-ecb5-45b0-bb2a-41c10cfb442f",
   "metadata": {},
   "source": [
    "# Task\n",
    "\n",
    "I task da svolgere sono:\n",
    "\n",
    "- [**phototaxis**](./phototaxis.ipynb): generazione dell'agente in un punto casuale della mappa con l'obiettivo di raggiungere la prima luce disponibile cercando di evitare muri.In caso l'agente non rilevi valori con il sensore di luce deve entrare in uno stato di esplorazione.\n",
    "- [**obstacle avoidance**](./obstacle-avoidance.ipynb): generazione dell'agente in un punto casuale dell'ambiente. L'obiettivo è quello di muoversi nello spazio senza toccare ostacoli e muri in un certo tempo definito.\n",
    "\n",
    "- [**exploration**](./exploration.ipynb): mgenerazione dell'agente in un punto casualee deell'ambiente. L'obiettivo è quello di massimizzare la copertura visitata dell'ambiente cercando di evitare gli ostacoli entro un certo numero di steps.\n",
    "\n",
    "## Modus operandi\n",
    "### Env generation\n",
    "è stata utile perchè\n",
    "### Training\n",
    "come viene svolta\n",
    "### Evaluation\n",
    "\n",
    "La valutazione delle performance dei `QAgent` e `DQAgent` viene effettuata tramite la funzione `evaluate`, la cui signature è la seguente:\n",
    "\n",
    "```python\n",
    "def evaluate(\n",
    "    env: PhototaxisEnv | ObstacleAvoidanceEnv | ExplorationEnv,\n",
    "    agents: dict[str, QAgent | DQAgent],\n",
    "    configs: list[str],\n",
    "    max_steps: int,\n",
    "    did_succeed: Callable[[float, bool, bool], bool],\n",
    "    window_size: int = 100,\n",
    ") -> dict\n",
    "```\n",
    "\n",
    "Viene passato l'`environment` specifico per il task, oltre a un dizionario con gli agenti e i relativi id.\n",
    "Questi vengono valutati su tutte le configurazioni fornite in `configs` per un numero di passi pari a `max_steps`.\n",
    "La _lambda_ `did_succeed` permette di capire se l'agente ha terminato l'episodio con un successo o un fallimento.\n",
    "Il parametro `window_size` permette di calcolare la `moving average reward`.\n",
    "\n",
    "La funzione supporta nativamente scenari **multi-agente** (da 1 a $N$) utilizzando dizionari per gestire stati e azioni indipendenti, isolando correttamente gli agenti che terminano l'episodio prima degli altri. Durante la valutazione la policy è deterministica (*greedy*).\n",
    "\n",
    "Le metriche restituite sono essenziali per diagnosticare la qualità dell'apprendimento:\n",
    "*   **`success_rate`** e **`successes_idx`**: indicano la robustezza dell'agente e quali specifiche configurazioni riesce a risolvere.\n",
    "*   **`median_steps_to_success`**: misura l'efficienza (velocità) nel raggiungere l'obiettivo.\n",
    "*   **`total_rewards`** e **`moving_avg_reward`**: valutano la performance cumulativa e la stabilità del comportamento durante l'episodio.\n",
    "*   **`td_losses`** (per DQAgent): monitora l'errore di stima dei valori $Q$, utile per rilevare incertezze o problemi di generalizzazione su stati non visti in training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04046ae4-dac9-4e30-81bf-56cb2ee36cfe",
   "metadata": {},
   "source": [
    "# Conclusioni\n",
    "\n",
    "## Commenti finali\n",
    "\n",
    "In conclusione il progetto ci ha permesso di integrare con successo il simulatore Scala con la gestione di agenti autonomi permettendoci di esplorare diverse tecniche del **Reinforcement Learning**.\n",
    "\n",
    "Tutti i task sono stati realizzati sia con tecniche di **Q-Learning** che **Deep Q-Learning**.\n",
    "I task di *phototaxis* e *obstacle avoidance* sono risultati più semplici, mentre *exploration* è risultato un problema molto più complesso.\n",
    "\n",
    "## Lavori futuri\n",
    "\n",
    "Allo stato attuale il simulatore e gli agenti autonomi funzionano sufficientemente bene per compiti semplici, si potrebbe però ampliare il simulatore con:\n",
    "\n",
    "- **hybrid control**: la possibilità di controllare sia in modo programmatico che tramite *Reinforcement Learning* gli agenti, permettendo quindi di ottenere performance migliori in compiti difficili da risolvere con tecniche di controllo autonomo;\n",
    "- **cooperative multi agents**: il sistema, allo stato attuale, supporta solo la valutazione multi agente sui diversi task, ma non l'addestramento multi agente. Uno sviluppo interessante potrebbe essere la realizzazione di task cooperativi, quali ad esempio il *flocking* o il *clustering*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
