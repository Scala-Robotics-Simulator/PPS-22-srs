{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e813879-0db7-4d71-8dcf-bf003c5e7567",
   "metadata": {},
   "source": [
    "# Obiettivo del progetto\n",
    "\n",
    "L'obiettivo del progetto è estendere il simulatore 2D di robot realizzato per il progetto di **Paradigmi di Programmazione e Sviluppo** per dotarlo di comportamenti appresi tramite tecniche di Reinforcement Learning, in particolare Q-learning e Deep Q-learning.\n",
    "\n",
    "Si è proposto di realizzare i seguenti tre comportamenti:\n",
    "- [**phototaxis**](./phototaxis.ipynb): movimento verso una sorgente luminosa;\n",
    "- [**obstacle avoidance**](./obstacle-avoidance.ipynb): movimento con evitamento di ostacoli;\n",
    "- [**exploration**](./exploration.ipynb): massimizzare l’esplorazione dell’ambiente evitando le collisioni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e4d84-9441-4dd7-800f-ed1aa29bf578",
   "metadata": {},
   "source": [
    "# Vincoli\n",
    "\n",
    "I vincoli posti sono:\n",
    "\n",
    "- ambiente: griglie 5×5, 20×20 e 30×30 m;\n",
    "- sensori: 8 prossimità + 8 luce + posizione + orientazione;\n",
    "- luci con raggio 0.2 m e irradiazione 5 m;\n",
    "- ostacoli rettangolari con dimensioni variabili."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6f832d-7a24-4a5a-a438-4882d82a6638",
   "metadata": {},
   "source": [
    "# Starting point\n",
    "Siamo partiti da un simulatore in grado di gestire un ambiente composto da N robot aventi un comportamento specificato da configurazione.\n",
    "\n",
    "Nell'ambiente sono presenti entità statiche, come ostacoli e luci e entità dinamiche, come altri robot.\n",
    "I robot sono in grado di percepire le altre entità attraverso i sensori di prossimità e di luce e possono interagire nell'ambiente attraverso gli attuatori tramite comportamenti pre-programmati. Il movimento dei robot segue un movimento differenziale in cui posono essere applicate delle velocità diverse alle due ruote.\n",
    "\n",
    "## Configurazione del simulatore (yaml)\n",
    "\n",
    "Per configurare il simulatore si possono utilizzare o file `yaml` oppure la GUI dedicata.\n",
    "\n",
    "L'utilizzo di file `yaml` permette di configurare il simulatore in modo semplice ed efficace. Un esempio di struttura è la seguente:\n",
    "\n",
    "```yaml\n",
    "simulation:\n",
    "  seed: 42\n",
    "environment:\n",
    "  width: 12\n",
    "  height: 10\n",
    "  entities:\n",
    "    - light:\n",
    "        position: [10, 5]\n",
    "        illuminationRadius: 6.0\n",
    "        intensity: 1.0\n",
    "        radius: 0.2\n",
    "        attenuation: 1.0\n",
    "    - obstacle:\n",
    "        position: [6, 5]\n",
    "        orientation: 0.0\n",
    "        width: 2.0\n",
    "        height: 6.0\n",
    "    - robot:\n",
    "        position: [2, 2]\n",
    "        orientation: 45.0\n",
    "        radius: 0.25\n",
    "        speed: 1.0\n",
    "        withProximitySensors: true\n",
    "        withLightSensors: true\n",
    "        behavior: Phototaxis\n",
    "```\n",
    "\n",
    "I file di configurazione sono stati utilizzati durante l'addestramento e la valutazione degli agenti.\n",
    "\n",
    "## Adattamento e modellazione del simulatore\n",
    "\n",
    "Il simulatore è stato esteso per integrare comportamenti basati su tecniche di Q-learning e Deep Q-learning rendendo i robot autonomi.\n",
    "Il simulatore è implementato interamente in `Scala`, mentre la parte di Reinforcement Learning è sviluppata in `Python`.\n",
    "\n",
    "### Comunicazione python-simulatore\n",
    "\n",
    "Per il collegamento tra i due linguaggi è stato adottato `gRPC`. Inoltre, è stato definita un’interfaccia di interazione modellata sullo stile di `PettingZoo`, così da mantenere coerenza con le principali librerie di Reinforcement Learning multi-agente.\n",
    "\n",
    "---\n",
    "\n",
    "### Simulatore lato Scala\n",
    "\n",
    "Di seguito viene fornita una descrizione delle modifiche apportate implementativamente al simulatore.\n",
    "\n",
    "#### Agente\n",
    "\n",
    "Invece che apportare delle modifiche ai robot esistenti si è preferito creare delle entità ad-hoc: gli agenti.\n",
    "Nel simulatore in Scala, l'agente ha le stesse caratteristiche del robot ad eccezione del `Behavior` in cui non associamo più un comportamento programmatico ma adesso è composto da `Reward`, `Termination` e `Truncation`.\n",
    "\n",
    "#### Reward\n",
    "\n",
    "La `Reward` è una funzione associata all'agente che permette di osservare lo stato dell'ambiente (precedente e corrente) e calcolare una ricompensa (bonus o penalità) adeguata al task che deve risolvere.\n",
    "\n",
    "Signature per la reward:\n",
    "\n",
    "```scala\n",
    "def evaluate(prev: BaseState, current: BaseState, entity: Agent, action: Action[?]): Double\n",
    "```\n",
    "\n",
    "Non sempre le informazioni dello stato precedente e corrente sono sufficienti per calcolare una ricompensa ottimale per il task in esame.\n",
    "È stata quindi introdotta una `StatefulReward` che permette di salvare informazioni da riutilizzare negli step successivi.\n",
    "\n",
    "Questa tipologia di reward è risultata particolarmente utile nel task di **Exploration** in quanto permette di salvare informazioni utili, tra cui le posizioni già visitate dagli agenti.\n",
    "\n",
    "#### Termination\n",
    "\n",
    "La `Termination` permette di concludere in maniera naturale la finestra di addestramento sia come situazione positiva o negativa, indipendentemente del numero massimo di step, se definiti.\n",
    "È stata modellata sulla base del task specifico da addestrare.\n",
    "\n",
    "La signature per la termination viene definita come:\n",
    "\n",
    "```scala\n",
    "def evaluate(prev: BaseState, current: BaseState, entity: Agent, action: Action[?]): Boolean\n",
    "```\n",
    "\n",
    "Come nel caso della `StatefulReward` si è reso necessario aggiungere anche il concetto di `StatefulTermination`, permettendo ad esempio nel task di **Exploration** di terminare il processo quando l'agente ha visitato una certa percentuale dell'ambiente.\n",
    "\n",
    "#### Truncation\n",
    "\n",
    "La `Truncation` è stata realizzata in modo equivalente alla `Termination`. Gestisce invece le condizioni di fallimento forzato.\n",
    "Il limite massimo di step è gestito principalmente lato `Python` per questo motivo non è stata utilizzata.\n",
    "\n",
    "---\n",
    "\n",
    "### Lato Python\n",
    "\n",
    "In `Python` è stato creato l'agente, l'ambiente osservabile da esso e gli algoritmi di training e valutazione.\n",
    "\n",
    "#### Descrizione dell'ambiente\n",
    "\n",
    "L'ambiente funziona da ponte con il simulatore e permette all'agente di interagire con esso svolgendo azioni e di percepire il mondo a lui circostante tramite le osservazioni.\n",
    "È stato modellato sulla base della libreria `gymnasium`, contiene quindi i seguenti metodi:\n",
    "\n",
    "- step:\n",
    "\n",
    "  ```python\n",
    "  def step(self, actions: dict) -> tuple[dict, dict, dict, dict, dict]\n",
    "  ```\n",
    "  \n",
    "  fa svolgere le azioni fornite agli agenti, effettuando un `tick` nel simulatore e restituisce quindi, per ogni agente:\n",
    "\n",
    "  - osservazioni;\n",
    "  - ricompense;\n",
    "  - terminazioni;\n",
    "  - troncamenti;\n",
    "  - informazioni aggiuntive.\n",
    "  \n",
    "- render:\n",
    "\n",
    "  ```python\n",
    "  def render(self, width: int = 800, height: int = 600) -> np.ndarray\n",
    "  ```\n",
    "\n",
    "  ritorna un immagine in formato RGB delle dimensioni indicate, contenente una rappresentazione dello stato del simulatore al momento della chiamata.\n",
    "\n",
    "- reset:\n",
    "\n",
    "  ```python\n",
    "  def reset(self, seed: int = 42) -> tuple[dict, dict]:\n",
    "  ```\n",
    "\n",
    "  riporta il simulatore alla configurazione iniziale e ritorna, per ogni agente:\n",
    "\n",
    "  - osservazioni;\n",
    "  - informazioni aggiuntive.\n",
    "\n",
    "Sono presenti, inoltre, i metodi:\n",
    "\n",
    "- `init`: utile a stabilire la connessione con il client e inizializzare l'ambiente con la configurazione;\n",
    "- `close`: per chiudere la connessione con `gRPC`.\n",
    "\n",
    "L'ambiente è stato creato come classe astratta `AbstractEnv` così che ciascuno potesse implementare il proprio modo per codificare e decodificare le osservazioni e azioni rispettivamente.\n",
    "\n",
    "```python\n",
    "@abstractmethod\n",
    "def _encode_observation(\n",
    "    self, proximity_values, light_values, position, orientation, visited_positions\n",
    "):\n",
    "    \"\"\"Encode the observation from proximity, light values, position, orientation and visited_positions\"\"\"\n",
    "    pass\n",
    "\n",
    "@abstractmethod\n",
    "def _decode_action(self, action):\n",
    "    \"\"\"Decode the action into the appropriate format\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "Le implementazioni specifiche dipendono da ciascun task, perciò verranno spiegate più nel dettaglio nei notebook referenziati successivamente.\n",
    "\n",
    "#### Agenti in Python\n",
    "\n",
    "In Python sono presenti due varianti dell'agente: il `QAgent` e il `DQAgent`.\n",
    "\n",
    "##### QAgent\n",
    "\n",
    "Il `QAgent` interagisce con un ambiente discreto e mantiene una Q-table inizializzata a zero. Le azioni vengono selezionate tramite politica epsilon-greedy: con probabilità ε si esplora casualmente, altrimenti si sceglie l’azione con il valore Q massimo. La Q-table viene aggiornata secondo la formula del Q-learning, considerando il fattore di apprendimento (α), il discount factor (γ) e la distinzione tra episodi terminati o troncati. Il tasso di esplorazione ε decresce esponenzialmente secondo la formula:\n",
    "\n",
    "$$\n",
    "\\epsilon = \\epsilon_{\\min} + (\\epsilon_{\\max} - \\epsilon_{\\min}) \\cdot e^{-\\text{decay} \\cdot \\text{episode}}\n",
    "$$\n",
    "\n",
    "Ulteriori funzionalità consentono di salvare e caricare lo stato dell’agente (Q-table e parametri) tramite file `.npz`.\n",
    "\n",
    "##### DQAgent\n",
    "\n",
    "Il `DQAgent` è progettato per apprendere politiche ottimali in ambienti complessi utilizzando due **Deep Q-Network**: la rete principale (`action_model`), che stima i valori Q delle azioni, e la rete target (`target_model`), aggiornata periodicamente per rendere l’apprendimento più stabile. L’agente interagisce con l’ambiente tramite una strategia epsilon-greedy e memorizza le esperienze raccolte in una replay memory, così da poterle riutilizzare più volte durante l’addestramento e ridurre la correlazione tra i campioni.\n",
    "\n",
    "Durante l’interazione con l’ambiente, le transizioni non vengono sempre memorizzate immediatamente. L’agente può infatti accumulare più passi consecutivi prima di salvarli nella replay memory, calcolando un ritorno che tiene conto di più ricompense future. Questo meccanismo, noto come **n-step return**, consente una propagazione più rapida dell’informazione sulle ricompense e rende il processo di apprendimento più stabile.\n",
    "\n",
    "Il tasso di esplorazione ε decresce esponenzialmente nel corso degli episodi, seguendo la stessa formulazione adottata per il `QAgent`, permettendo un passaggio graduale dall’esplorazione iniziale allo sfruttamento delle azioni apprese.\n",
    "\n",
    "Funzionalità principali del DQAgent:\n",
    "\n",
    "- **Selezione dell’azione (`choose_action`)**\n",
    "  L’agente sceglie le azioni secondo una politica epsilon-greedy, esplorando casualmente con probabilità ε o selezionando l’azione con il valore Q più elevato stimato dalla rete principale.\n",
    "\n",
    "- **Memorizzazione delle transizioni (`store_transition`)**  \n",
    "  Le esperienze vengono raccolte e, dopo aver eventualmente accumulato più passi consecutivi, salvate nella replay memory sotto forma di transizioni arricchite da ritorni multi-step.\n",
    "\n",
    "- **Campionamento mini-batch (`get_random_batch` e `get_random_batch_from_replay_memory`)**  \n",
    "  Dalla replay memory vengono estratti mini-batch casuali, riducendo la correlazione tra esempi e migliorando la stabilità dell’addestramento.\n",
    "\n",
    "- **Aggiornamento della rete principale (`dqn_update`)**\n",
    "  I pesi della rete principale vengono aggiornati minimizzando la perdita TD (Temporal Difference), calcolata confrontando i valori Q stimati con i target ottenuti dalle transizioni memorizzate.\n",
    "\n",
    "- **Aggiornamento della rete target (`update_target_model`)**  \n",
    "  La rete target viene sincronizzata periodicamente con la rete principale per limitare oscillazioni e instabilità durante l’apprendimento.\n",
    "\n",
    "- **Decadimento del tasso di esplorazione (`decay_epsilon`)**  \n",
    "  Il valore di ε diminuisce progressivamente nel tempo, favorendo lo sfruttamento delle politiche apprese.\n",
    "\n",
    "- **Inizializzazione della memoria (`simple_dqn_replay_memory_init`)**  \n",
    "  Prima dell’addestramento, la replay memory viene popolata con transizioni casuali per garantire un avvio stabile del processo di apprendimento.\n",
    "\n",
    "- **Calcolo della perdita TD singola (`compute_td_loss`)**  \n",
    "  Consente di valutare l’errore TD su singole transizioni, risultando utile per analisi e debugging.\n",
    "\n",
    "- **Salvataggio e caricamento (`save` e `load`)**  \n",
    "  Permette di salvare e ripristinare modelli, parametri e stato dell’agente, rendendo possibile interrompere e riprendere l’addestramento senza perdita di informazioni.\n",
    "\n",
    "###### Deep Q-Network\n",
    "\n",
    "La `DQNetwork` è la rete neurale utilizzata per stimare i valori Q delle azioni in un dato stato, permettendo all’agente di apprendere politiche ottimali anche in ambienti complessi e a spazi di stato continui.\n",
    "\n",
    "- **Input:** vettore di caratteristiche dello stato (`input_count`).  \n",
    "- **Hidden layers:** uno o più strati densi con attivazione ReLU; numero di neuroni definito da `neuron_count_per_hidden_layer`.  \n",
    "- **Output:** uno strato con un neurone per ciascuna azione (`action_count`), che produce i valori Q.  \n",
    "- **Ottimizzazione e perdita:** loss **MSE** e ottimizzatore **Adam** per aggiornamenti stabili.  \n",
    "\n",
    "Funzionalità principali: **predizione valori Q (`predict`)**, **aggiornamento pesi (`update_weights`)** con la rete target e possibilità di visualizzare la struttura della rete tramite summary o diagrammi.\n",
    "\n",
    "---\n",
    "\n",
    "# Metodo risolutivo\n",
    "\n",
    "Per la realizzazione dei task si è seguito un processo iterativo.\n",
    "Inizialmente sono stati generati gli ambienti di training e test, successivamente vengono effettuati gli addestramenti e le valutazioni.\n",
    "\n",
    "Sulla base dei risultati ottenuti dai primi training, per un numero di epoche più ridotto, vengono analizzati gli errori che gli agenti compiono; queste analisi permettono iterativamente di modificare l'approccio risolutivo, la _reward_ e le funzioni di _truncation_ e _termination_ in modo che meglio riflettano i requisiti del task specifico.\n",
    "\n",
    "Si è prima realizzata l'implementazione riguardante il Q-Learning tabellare, e solo successivamente il Deep-Q-Learning.\n",
    "\n",
    "Affrontare inizialmente i task con i `QAgent` ha permesso di individuare rapidamente le criticità ricorrenti legate alla funzione di reward, che rendevano l’addestramento poco efficace. Successivamente, l’addestramento dei `DQAgent` è risultato più agevole, grazie alla conoscenza preventiva delle strategie per aggirare i problemi più comuni.\n",
    "\n",
    "Di seguito sono spiegate tutte le fasi principali necessarie alla risoluzione dei task.\n",
    "\n",
    "## Generazione degli ambienti\n",
    "\n",
    "Per la generazione degli ambienti di training e test è stato utilizzato lo script `generate_environments.py`([link al file](../../scripts/generate_environments.py)).\n",
    "\n",
    "Lo script permette di configurare parametri riguardanti:\n",
    "\n",
    "- **dimensioni ambiente**: `--width` e `--height`\n",
    "- **ostacoli**:\n",
    "  - Numero: `--obstacle-min-num` e `--obstacle-max-num`;\n",
    "  - Dimensioni: `--obstacle-min-size` e `--obstacle-max-size`;\n",
    "- **luci**:\n",
    "  - Numero: `--light-min-num` e `--light-max-num`;\n",
    "  - Parametri di illuminazione automatici.\n",
    "\n",
    "Esempio di utilizzo:\n",
    "\n",
    "```bash\n",
    "python generate_environments.py \\\n",
    "  --num 10 \\\n",
    "  --config-root resources generated obstacle-avoidance \\\n",
    "  --width 10 --height 10 \\\n",
    "  --obstacle-min-num 4 --obstacle-max-num 15 \\\n",
    "  --obstacle-min-size 0.5 --obstacle-max-size 8.0 \\\n",
    "  --light-min-num 0 --light-max-num 0\n",
    "```\n",
    "\n",
    "Lo script genera file `yaml` validi controllando la validità dell'ambiente tramite il simulatore. Utilizza la libreria `environment_generator.py`([link al file](../../scripts/lib/environment_generator.py)), la quale verifica che gli ambienti generati siano validi.\n",
    "\n",
    "## Training\n",
    "\n",
    "La fase di addestramento utilizza due approcci differenti in base alla complessità del task: **Q-learning tabulare** e\n",
    "**Deep Q-learning**.\n",
    "Il training viene eseguito tramite script `Python` dedicati (`train-qagent.py` e `train-dqagent.py`), che si occupano di\n",
    "gestire l’interazione tra agente e simulatore tramite `gRPC`, l’esecuzione degli episodi e il salvataggio dei dati\n",
    "necessari alla successiva fase di valutazione.\n",
    "\n",
    "Gli script permettono inoltre di configurare i principali parametri di training (numero di episodi, step massimi,\n",
    "frequenza dei checkpoint) e di riprendere un addestramento precedente, rendendo gli esperimenti ripetibili e\n",
    "controllabili.\n",
    "\n",
    "### Q-learning\n",
    "\n",
    "L’addestramento con **Q-learning** è utilizzato per task con spazio degli stati discreto e di dimensione limitata.  \n",
    "In questo caso l’agente mantiene una **Q-table**, che associa ad ogni coppia stato–azione un valore di utilità appreso\n",
    "durante l’interazione con l’ambiente.\n",
    "\n",
    "Durante il training l’agente segue una politica **epsilon-greedy**, che bilancia esplorazione e sfruttamento:\n",
    "inizialmente vengono provate molte azioni diverse, mentre con il progredire degli episodi l’agente tende a sfruttare le\n",
    "azioni che hanno prodotto le ricompense migliori.\n",
    "Il parametro `epsilon` viene quindi ridotto progressivamente tramite decadimento esponenziale.\n",
    "\n",
    "Lo script di training gestisce:\n",
    "\n",
    "- l’esecuzione degli episodi di addestramento;\n",
    "- l’aggiornamento della Q-table in base alle ricompense osservate;\n",
    "- il salvataggio periodico della Q-table e dei parametri dell’agente tramite checkpoint.\n",
    "\n",
    "### Deep Q-learning\n",
    "\n",
    "Per task più complessi, caratterizzati da osservazioni continue o da uno spazio degli stati più ampio, viene utilizzato\n",
    "**Deep Q-learning**.  \n",
    "In questo caso la funzione Q non è rappresentata da una tabella, ma viene approssimata tramite una **rete neurale**.\n",
    "\n",
    "Durante il training l’agente memorizza le esperienze sotto forma di transizioni\n",
    "`(stato, azione, ricompensa, stato successivo)` all’interno di una **replay memory**.  \n",
    "Le reti neurali vengono aggiornate campionando mini-batch casuali da questa memoria, riducendo la correlazione tra\n",
    "esperienze consecutive e rendendo l’apprendimento più stabile.\n",
    "\n",
    "Per migliorare ulteriormente la stabilità, il training utilizza due reti distinte:\n",
    "\n",
    "- una **action network**, aggiornata frequentemente;\n",
    "- una **target network**, aggiornata periodicamente.\n",
    "\n",
    "Anche in questo caso l’agente utilizza una politica **epsilon-greedy**, con una riduzione graduale dell’esplorazione nel\n",
    "corso del training.\n",
    "La convergenza viene monitorata tramite la media mobile delle reward e, se questa supera una soglia prefissata, il\n",
    "training viene terminato anticipatamente (**early stopping**).\n",
    "\n",
    "Durante l’addestramento vengono salvati checkpoint periodici contenenti i pesi delle reti neurali e i parametri\n",
    "dell’agente, che vengono poi utilizzati nella fase di valutazione.\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "La valutazione delle performance dei `QAgent` e `DQAgent` viene effettuata tramite la funzione `evaluate`, la cui signature è la seguente:\n",
    "\n",
    "```python\n",
    "def evaluate(\n",
    "    env: PhototaxisEnv | ObstacleAvoidanceEnv | ExplorationEnv,\n",
    "    agents: dict[str, QAgent | DQAgent],\n",
    "    configs: list[str],\n",
    "    max_steps: int,\n",
    "    did_succeed: Callable[[float, bool, bool], bool],\n",
    "    window_size: int = 100,\n",
    ") -> dict\n",
    "```\n",
    "\n",
    "Viene passato l'`environment` specifico per il task, oltre a un dizionario con gli agenti e i relativi id.\n",
    "Questi vengono valutati su tutte le configurazioni fornite in `configs` per un numero di passi pari a `max_steps`.\n",
    "La _lambda_ `did_succeed` permette di capire se l'agente ha terminato l'episodio con successo o fallimento.\n",
    "Il parametro `window_size` permette di calcolare la `moving average reward`.\n",
    "\n",
    "La funzione supporta nativamente scenari **multi-agente** (da 1 a $N$) utilizzando dizionari per gestire stati e azioni indipendenti, isolando correttamente gli agenti che terminano l'episodio prima degli altri. Durante la valutazione la policy è deterministica (_greedy_).\n",
    "\n",
    "Le metriche restituite sono essenziali per diagnosticare la qualità dell'apprendimento:\n",
    "\n",
    "- **`success_rate`** e **`successes_idx`**: indicano la robustezza dell'agente e quali specifiche configurazioni riesce a risolvere;\n",
    "- **`median_steps_to_success`**: misura l'efficienza (velocità) nel raggiungere l'obiettivo;\n",
    "- **`total_rewards`** e **`moving_avg_reward`**: valutano la performance cumulativa e la stabilità del comportamento durante l'episodio;\n",
    "- **`td_losses`** (per `DQAgent`): monitora l'errore di stima dei valori $Q$, utile per rilevare incertezze o problemi di generalizzazione su stati non visti in training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926386b5-6288-4565-89b7-189d6c464b66",
   "metadata": {},
   "source": [
    "# Per eseguire i notebook relativi ai task\n",
    "\n",
    "Per eseguire il simulatore è necessario installare le dipendenze python (`uv pip install -r pyproject.toml`) e generare le definizioni di protobuf, quindi avviare il simulatore con docker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c89496-7a28-4dee-a3b9-902bb6479235",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../../.. && ./create_proto_definitions.sh\n",
    "!docker run ghcr.io/scala-robotics-simulator/pps-22-srs:v2.0.0\n",
    "# Per poter eseguire i notebook relativi ai task è necessario avviare il simulatore prima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91598e86-bf62-4e0d-a7ac-3c05eb0c0e5e",
   "metadata": {},
   "source": [
    "# Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5798b3-ecb5-45b0-bb2a-41c10cfb442f",
   "metadata": {},
   "source": [
    "I task da svolgere sono:\n",
    "\n",
    "- [**phototaxis**](./phototaxis.ipynb): generazione dell'agente in un punto casuale della mappa con l'obiettivo di raggiungere la prima luce disponibile cercando di evitare muri. In caso l'agente non rilevi valori con il sensore di luce deve entrare in uno stato di esplorazione.\n",
    "- [**obstacle avoidance**](./obstacle-avoidance.ipynb): generazione dell'agente in un punto casuale dell'ambiente. L'obiettivo è quello di muoversi nello spazio senza toccare ostacoli e muri in un certo tempo definito.\n",
    "- [**exploration**](./exploration.ipynb): generazione dell'agente in un punto casuale dell'ambiente. L'obiettivo è quello di massimizzare la copertura visitata dell'ambiente cercando di evitare gli ostacoli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04046ae4-dac9-4e30-81bf-56cb2ee36cfe",
   "metadata": {},
   "source": [
    "# Conclusioni\n",
    "\n",
    "## Commenti finali\n",
    "\n",
    "Il progetto ci ha permesso di esplorare tecniche di controllo robotico automatico, attraverso il **Reinforcement Learning**, in particolare _Q-learning_ e _Deep Q-learning_.\n",
    "\n",
    "Realizzando interamente sia il simulatore che il sistema di comunicazione tra esso e gli agenti, abbiamo potuto sperimentare in un ambiente controllato l'efficacia delle tecniche di apprendimento automatico applicate al controllo robotico.\n",
    "\n",
    "Tutti i task sono stati realizzati sia con tecniche di **Q-Learning** che **Deep Q-Learning**.\n",
    "I task di *phototaxis* e *obstacle avoidance* sono risultati più semplici, mentre *exploration* è risultato un problema molto più complesso.\n",
    "\n",
    "**Caratteristiche salienti del progetto:**\n",
    "\n",
    "1. **architettura ibrida Scala-Python**: utilizzare `gRPC` per la comunicazione tra il simulatore e gli agenti ha permesso di sfruttare le potenzialità di entrambi i linguaggi; `Scala` per la robustezza, immutabilità e performance del simulatore, `Python` per il ricco ecosistema di librerie di machine learning e deep learning;\n",
    "\n",
    "2. **reward engineering**: l'utilizzo di funzioni di ricompensa stateless e stateful ha permesso di realizzare funzioni adatte ai diversi scenari di apprendimento, migliorando la velocità di convergenza e la qualità delle politiche apprese;\n",
    "\n",
    "3. **generazione ambienti**: lo script per generare ambienti casuali ha permesso di realizzare esperimenti su larga scala, testando la robustezza delle politiche apprese in ambienti diversi.\n",
    "\n",
    "**Confronto tra Q-learning e Deep Q-learning:**\n",
    "\n",
    "- **Q-learning**:\n",
    "  - Vantaggi:\n",
    "    - interpretabilità;\n",
    "    - semplicità di implementazione;\n",
    "    - bassa richiesta computazionale.\n",
    "  - Svantaggi:\n",
    "    - scalabilità limitata a spazi di stato piccoli;\n",
    "    - difficoltà nell'estrazione di caratteristiche rilevanti dagli stati complessi.\n",
    "\n",
    "- **Deep Q-learning**:\n",
    "  - Vantaggi:\n",
    "    - capacità di gestire spazi di stato complessi e di grandi dimensioni;\n",
    "    - apprendimento automatico delle caratteristiche rilevanti dagli stati.\n",
    "  - Svantaggi:\n",
    "    - maggiore complessità di implementazione;\n",
    "    - elevata richiesta computazionale;\n",
    "    - reward engineering più complesso.\n",
    "\n",
    "\n",
    "## Lavori futuri\n",
    "\n",
    "Allo stato attuale il simulatore e gli agenti autonomi funzionano sufficientemente bene per compiti semplici, si potrebbe però ampliare il simulatore con:\n",
    "\n",
    "- **hybrid control**: la possibilità di controllare sia in modo programmatico che tramite *Reinforcement Learning* gli agenti, permettendo quindi di ottenere performance migliori in compiti difficili da risolvere con tecniche di controllo autonomo;\n",
    "- **cooperative multi agents**: il sistema, allo stato attuale, supporta solo la valutazione multi agente sui diversi task, ma non l'addestramento multi agente. Uno sviluppo interessante potrebbe essere la realizzazione di task cooperativi, quali ad esempio il *flocking* o il *clustering*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
