{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning for Phototaxis Task - Original Complex Reward\n",
    "\n",
    "This notebook uses the **Phototaxis** (original fixed) reward variant.\n",
    "\n",
    "**Reward Components:**\n",
    "- Progress reward (+10× distance improvement)\n",
    "- Proximity reward (exponential, rewards being close)\n",
    "- Goal bonus (+100 when reaching light)\n",
    "- Obstacle penalty (-10× proximity violation)\n",
    "- Movement penalties (spinning -1.0, oscillation -0.1×)\n",
    "- Forward bonus (+0.2)\n",
    "- Survival reward (logarithmic, small)\n",
    "\n",
    "**Best for:** Fine-grained control, advanced experiments\n",
    "\n",
    "**Note:** This is the most complex reward with many tunable components."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T11:32:36.049403Z",
     "start_time": "2025-11-05T11:32:36.044854Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T11:32:36.210329Z",
     "start_time": "2025-11-05T11:32:36.205225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T11:32:36.229886Z",
     "start_time": "2025-11-05T11:32:36.219793Z"
    }
   },
   "source": "from tqdm import trange",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T11:32:37.483026Z",
     "start_time": "2025-11-05T11:32:37.304430Z"
    }
   },
   "source": [
    "from environment.qlearning.phototaxis_env import PhototaxisEnv\n",
    "from utils.reader import get_yaml_path, read_file"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T11:32:37.536927Z",
     "start_time": "2025-11-05T11:32:37.529141Z"
    }
   },
   "source": [
    "\n",
    "from agent.qagent import QAgent\n",
    "from training.qlearning import QLearning"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T11:32:38.784530Z",
     "start_time": "2025-11-05T11:32:38.744912Z"
    }
   },
   "source": [
    "import pygame\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Simulator"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T11:32:39.389531Z",
     "start_time": "2025-11-05T11:32:39.373406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "server_address = \"localhost:50051\"\n",
    "client_name = \"PhototaxisRLClient\"\n",
    "env = PhototaxisEnv(server_address, client_name)\n",
    "env.connect_to_client()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-05 12:32:39,375 — INFO — [PhototaxisEnv] Light: light8 (+no_light=False) -> 8 states\n",
      "2025-11-05 12:32:39,376 — INFO — [PhototaxisEnv] Prox: prox4 -> 4 states\n",
      "2025-11-05 12:32:39,377 — INFO — [PhototaxisEnv] Bins: light=1, prox=2\n",
      "2025-11-05 12:32:39,377 — INFO — [PhototaxisEnv] Total discrete states: 64\n",
      "2025-11-05 12:32:39,387 — INFO — ✓ Connected to localhost:50051\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Configuration - Original Complex Reward"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T11:28:15.044311Z",
     "start_time": "2025-11-05T11:28:15.041802Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T11:28:15.376338Z",
     "start_time": "2025-11-05T11:28:15.374315Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T11:32:42.369003Z",
     "start_time": "2025-11-05T11:32:42.351857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from environment.qlearning.phototaxis_env import PhototaxisEnv\n",
    "from utils.reader import get_yaml_path, read_file\n",
    "from agent.qagent import QAgent\n",
    "from training.qlearning import QLearning\n",
    "import pygame\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "env = PhototaxisEnv(\n",
    "    \"localhost:50051\", \"ptx7\",\n",
    "    light_direction=\"light8\",\n",
    "    light_has_no_light_state=False,\n",
    "    prox_direction=\"front_min\",\n",
    "    prox_intensity_bins = 2,\n",
    ")\n",
    "\n",
    "env.connect_to_client()\n",
    "\n",
    "config_path = get_yaml_path(\"resources\", \"configurations\", \"../../src/scripts/resources/generated/phototaxis/conf/environment_0001.yml\")\n",
    "config = read_file(config_path)\n",
    "env.init(config)\n",
    "# print(config)\n",
    "episodes = 10\n",
    "steps = 1000\n",
    "agent = QAgent(env, episodes = episodes)\n",
    "agentId = \"00000000-0000-0000-0000-000000000001\"\n",
    "agents = { agentId: agent }"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-05 12:32:42,354 — INFO — [PhototaxisEnv] Light: light8 (+no_light=False) -> 8 states\n",
      "2025-11-05 12:32:42,355 — INFO — [PhototaxisEnv] Prox: front_min -> 1 states\n",
      "2025-11-05 12:32:42,355 — INFO — [PhototaxisEnv] Bins: light=1, prox=2\n",
      "2025-11-05 12:32:42,355 — INFO — [PhototaxisEnv] Total discrete states: 16\n",
      "2025-11-05 12:32:42,357 — INFO — ✓ Connected to localhost:50051\n",
      "\n",
      "2025-11-05 12:32:42,365 — INFO — ✓ Initialization successful\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training Parameters"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training Loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T11:32:46.341861Z",
     "start_time": "2025-11-05T11:32:46.330863Z"
    }
   },
   "source": [
    "def run_episodes(\n",
    "    episode_count, \n",
    "    episode_max_steps, \n",
    "    render=False, \n",
    "    fps=60,\n",
    "    checkpoint_interval=None,\n",
    "    checkpoint_path=\"checkpoints/phototaxis_agent\",\n",
    "    load_checkpoint=None,\n",
    "    start_episode=0\n",
    "):\n",
    "    import os\n",
    "    \n",
    "    # Load existing agent if specified\n",
    "    if load_checkpoint:\n",
    "        for agent_id, agent_obj in agents.items():\n",
    "            agent_obj.load(load_checkpoint)\n",
    "            print(f\"Loaded agent from {load_checkpoint}\")\n",
    "    \n",
    "    # Create checkpoint directory if needed\n",
    "    if checkpoint_interval:\n",
    "        os.makedirs(os.path.dirname(checkpoint_path) if os.path.dirname(checkpoint_path) else \".\", exist_ok=True)\n",
    "    \n",
    "    running = True\n",
    "    paused = False\n",
    "    current_fps = fps\n",
    "    \n",
    "    # Initialize pygame for rendering\n",
    "    if render:\n",
    "        pygame.init()\n",
    "        screen = pygame.display.set_mode((800, 600))\n",
    "        pygame.display.set_caption(f\"Q-Learning Phototaxis - FPS: {current_fps}\")\n",
    "        clock = pygame.time.Clock()\n",
    "        \n",
    "        # Font for displaying info\n",
    "        try:\n",
    "            font = pygame.font.Font(None, 24)\n",
    "            info_font = pygame.font.Font(None, 20)\n",
    "        except:\n",
    "            font = None\n",
    "            info_font = None\n",
    "    \n",
    "    try:\n",
    "        for ep_idx in trange(episode_count, desc=\"Training\", unit=\"ep\"):\n",
    "            actual_episode = start_episode + ep_idx\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            total_reward = {agentId: 0}\n",
    "            step_count = 0\n",
    "            \n",
    "            while not done and step_count < episode_max_steps:\n",
    "                # Handle pygame events\n",
    "                if render:\n",
    "                    for event in pygame.event.get():\n",
    "                        if event.type == pygame.QUIT:\n",
    "                            running = False\n",
    "                        elif event.type == pygame.KEYDOWN:\n",
    "                            if event.key == pygame.K_ESCAPE or event.key == pygame.K_q:\n",
    "                                running = False\n",
    "                            elif event.key == pygame.K_SPACE:\n",
    "                                paused = not paused\n",
    "                            elif event.key == pygame.K_UP:\n",
    "                                current_fps = min(240, current_fps + 10)\n",
    "                                pygame.display.set_caption(f\"Q-Learning Phototaxis - FPS: {current_fps}\")\n",
    "                            elif event.key == pygame.K_DOWN:\n",
    "                                current_fps = max(10, current_fps - 10)\n",
    "                                pygame.display.set_caption(f\"Q-Learning Phototaxis - FPS: {current_fps}\")\n",
    "                            elif event.key == pygame.K_s:\n",
    "                                # Manual save\n",
    "                                for agent_id, agent_obj in agents.items():\n",
    "                                    save_path = f\"{checkpoint_path}_manual_ep{actual_episode}\"\n",
    "                                    agent_obj.save(save_path)\n",
    "                                    print(f\"\\n[Manual Save] Episode {actual_episode}\")\n",
    "                \n",
    "                if not running:\n",
    "                    break\n",
    "                \n",
    "                # Skip step if paused\n",
    "                if paused and render:\n",
    "                    pygame.time.wait(100)\n",
    "                    continue\n",
    "                \n",
    "                # Choose and execute actions\n",
    "                actions = {\n",
    "                    k: agents[k].choose_action(v, epsilon_greedy=not render) \n",
    "                    for k, v in obs.items()\n",
    "                }\n",
    "                next_obs, rewards, terminateds, truncateds, _ = env.step(actions)\n",
    "                \n",
    "                done = terminateds[agentId] or truncateds[agentId]\n",
    "                \n",
    "                # Update Q-table (only during training)\n",
    "                if not render:\n",
    "                    for k in next_obs.keys():\n",
    "                        agents[k].update_q(obs[k], actions[k], rewards[k], next_obs[k], done)\n",
    "                        total_reward[k] += rewards[k]\n",
    "                else:\n",
    "                    # Track reward even during rendering\n",
    "                    total_reward[agentId] += rewards[agentId]\n",
    "                \n",
    "                obs = next_obs\n",
    "                \n",
    "                # Render visualization\n",
    "                if render:\n",
    "                    rgb_array = env.render()\n",
    "                    surface = pygame.surfarray.make_surface(np.transpose(rgb_array, (1, 0, 2)))\n",
    "                    screen.blit(surface, (0, 0))\n",
    "                    \n",
    "                    # Display info overlay\n",
    "                    if font and info_font:\n",
    "                        info_texts = [\n",
    "                            f\"Episode: {actual_episode + 1}/{start_episode + episode_count}\",\n",
    "                            f\"Step: {step_count}/{episode_max_steps}\",\n",
    "                            f\"Reward: {total_reward[agentId]:.2f}\",\n",
    "                            f\"Epsilon: {agents[agentId].epsilon:.4f}\",\n",
    "                            f\"FPS: {current_fps} (↑/↓ to adjust)\",\n",
    "                            f\"{'PAUSED' if paused else 'SPACE: Pause'}\"\n",
    "                        ]\n",
    "                        \n",
    "                        y_offset = 10\n",
    "                        for text in info_texts:\n",
    "                            color = (255, 255, 0) if paused else (255, 255, 255)\n",
    "                            text_surface = info_font.render(text, True, color, (0, 0, 0))\n",
    "                            screen.blit(text_surface, (10, y_offset))\n",
    "                            y_offset += 25\n",
    "                    \n",
    "                    pygame.display.flip()\n",
    "                    clock.tick(current_fps)\n",
    "                \n",
    "                step_count += 1\n",
    "            \n",
    "            if not running:\n",
    "                print(\"\\nTraining interrupted by user\")\n",
    "                break\n",
    "            \n",
    "            # Decay epsilon after episode\n",
    "            for agent_obj in agents.values():\n",
    "                agent_obj.decay_epsilon(actual_episode)\n",
    "            \n",
    "            # Save checkpoint at intervals\n",
    "            if checkpoint_interval and (ep_idx + 1) % checkpoint_interval == 0:\n",
    "                for agent_id, agent_obj in agents.items():\n",
    "                    save_path = f\"{checkpoint_path}_ep{actual_episode + 1}\"\n",
    "                    agent_obj.save(save_path)\n",
    "                    print(f\"\\n[Checkpoint] Saved at episode {actual_episode + 1}\")\n",
    "    \n",
    "    finally:\n",
    "        # Cleanup pygame\n",
    "        if render:\n",
    "            pygame.quit()\n",
    "        \n",
    "        # Final save if checkpointing was enabled\n",
    "        if checkpoint_interval and running:\n",
    "            for agent_id, agent_obj in agents.items():\n",
    "                save_path = f\"{checkpoint_path}_final\"\n",
    "                agent_obj.save(save_path)\n",
    "                print(f\"\\n[Final Save] Training complete\")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Train the Agent with Checkpoints"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T11:35:10.972918Z",
     "start_time": "2025-11-05T11:32:59.929014Z"
    }
   },
   "source": [
    "run_episodes(\n",
    "    episode_count=200,\n",
    "    episode_max_steps=1200,\n",
    "    # load_checkpoint=\"checkpoints/phototaxis_classic_final\",\n",
    "    start_episode=0,\n",
    "    checkpoint_interval=100,\n",
    "    checkpoint_path=\"checkpoints/phototaxis_classic\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████▉     | 99/200 [01:09<00:23,  4.23ep/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-05 12:34:10,166 — INFO — Agent saved to checkpoints/phototaxis_classic_ep100.npz (Total episodes: 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 100/200 [01:10<00:23,  4.24ep/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved at episode 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████▉| 199/200 [02:10<00:00,  1.16ep/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-05 12:35:10,967 — INFO — Agent saved to checkpoints/phototaxis_classic_ep200.npz (Total episodes: 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [02:11<00:00,  1.53ep/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Checkpoint] Saved at episode 200\n",
      "2025-11-05 12:35:10,969 — INFO — Agent saved to checkpoints/phototaxis_classic_final.npz (Total episodes: 200)\n",
      "\n",
      "[Final Save] Training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Trained Agent\n",
    "\n",
    "**Keyboard Controls (during render):**\n",
    "- `↑/↓`: Adjust FPS (10-240)\n",
    "- `SPACE`: Pause/Resume\n",
    "- `S`: Manual checkpoint save\n",
    "- `ESC/Q`: Quit training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-05T11:37:45.918223Z",
     "start_time": "2025-11-05T11:37:38.684291Z"
    }
   },
   "source": [
    "# Evaluate with rendering (use ↑/↓ to adjust speed, SPACE to pause)\n",
    "# run_episodes(1, 10000, render=True, load_checkpoint=\"../scripts/resources/generated/phototaxis/checkpoints/run_ep5421\")\n",
    "\n",
    "# # Evaluate with rendering (use ↑/↓ to adjust speed, SPACE to pause)\n",
    "\n",
    "run_episodes(1, 10000, render=True, load_checkpoint=\"checkpoints/phototaxis_classic_ep100\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-05 12:37:38,686 — INFO — Agent loaded from checkpoints/phototaxis_classic_ep100.npz\n",
      "2025-11-05 12:37:38,687 — INFO —   Q-table shape: (16, 4)\n",
      "2025-11-05 12:37:38,687 — INFO —   Current epsilon: 0.0010\n",
      "2025-11-05 12:37:38,687 — INFO —   Total episodes trained: 100\n",
      "Loaded agent from checkpoints/phototaxis_classic_ep100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1/1 [00:07<00:00,  7.08s/ep]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||## Resume Training from Checkpoint (Optional)\n",
    "\n",
    "If you want to continue training from a saved checkpoint, run this cell."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-04T22:36:15.750662Z",
     "start_time": "2025-11-04T22:36:15.672729Z"
    }
   },
   "source": [
    "# Example: Resume from episode 200 and train for 100 more episodes\n",
    "run_episodes(\n",
    "    episode_count=100,\n",
    "    episode_max_steps=steps,\n",
    "    checkpoint_interval=25,\n",
    "    checkpoint_path=\"checkpoints/phototaxis_agent\",\n",
    "    load_checkpoint=\"checkpoints/phototaxis_agent_ep1313\",\n",
    "    start_episode=200,\n",
    ")"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoints/phototaxis_agent_ep1313.npz'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mFileNotFoundError\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Example: Resume from episode 200 and train for 100 more episodes\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m \u001B[43mrun_episodes\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepisode_count\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepisode_max_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcheckpoint_interval\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m25\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcheckpoint_path\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcheckpoints/phototaxis_agent\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mload_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcheckpoints/phototaxis_agent_ep1313\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstart_episode\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m200\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 16\u001B[39m, in \u001B[36mrun_episodes\u001B[39m\u001B[34m(episode_count, episode_max_steps, render, fps, checkpoint_interval, checkpoint_path, load_checkpoint, start_episode)\u001B[39m\n\u001B[32m     14\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m load_checkpoint:\n\u001B[32m     15\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m agent_id, agent_obj \u001B[38;5;129;01min\u001B[39;00m agents.items():\n\u001B[32m---> \u001B[39m\u001B[32m16\u001B[39m         \u001B[43magent_obj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mload_checkpoint\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     17\u001B[39m         \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mLoaded agent from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mload_checkpoint\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     19\u001B[39m \u001B[38;5;66;03m# Create checkpoint directory if needed\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Github\\university\\first_year\\second_semester\\PPS-22-srs\\python\\src\\notebooks\\..\\agent\\qagent.py:147\u001B[39m, in \u001B[36mQAgent.load\u001B[39m\u001B[34m(self, filepath)\u001B[39m\n\u001B[32m    139\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mload\u001B[39m(\u001B[38;5;28mself\u001B[39m, filepath: \u001B[38;5;28mstr\u001B[39m):\n\u001B[32m    140\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Load the Q-table and agent parameters from a file.\u001B[39;00m\n\u001B[32m    141\u001B[39m \n\u001B[32m    142\u001B[39m \u001B[33;03m    Parameters\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    145\u001B[39m \u001B[33;03m        Path to load the agent state from (without extension).\u001B[39;00m\n\u001B[32m    146\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m147\u001B[39m     data = \u001B[43mnp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43mf\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mfilepath\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[33;43m.npz\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m    148\u001B[39m     \u001B[38;5;28mself\u001B[39m.Q = data[\u001B[33m\"\u001B[39m\u001B[33mq_table\u001B[39m\u001B[33m\"\u001B[39m]\n\u001B[32m    149\u001B[39m     \u001B[38;5;28mself\u001B[39m.epsilon = \u001B[38;5;28mfloat\u001B[39m(data[\u001B[33m\"\u001B[39m\u001B[33mepsilon\u001B[39m\u001B[33m\"\u001B[39m])\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Github\\university\\first_year\\second_semester\\PPS-22-srs\\python\\.venv\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:454\u001B[39m, in \u001B[36mload\u001B[39m\u001B[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[39m\n\u001B[32m    452\u001B[39m     own_fid = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m    453\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m454\u001B[39m     fid = stack.enter_context(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfspath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[32m    455\u001B[39m     own_fid = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    457\u001B[39m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[31mFileNotFoundError\u001B[39m: [Errno 2] No such file or directory: 'checkpoints/phototaxis_agent_ep1313.npz'"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Check Episodes from checkpoint"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "d = np.load('checkpoints/phototaxis_classic_final.npz')\n",
    "total_trained = d.get(\"total_episodes_trained\", 0)\n",
    "print(f\"Resume from episode: {total_trained}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Inspect Q-Table Statistics\n\nAfter training, check how much of the state space was explored."
  },
  {
   "cell_type": "code",
   "source": "print(f\"Q-table shape: {agent.Q.shape}\")\nprint(f\"Non-zero entries: {np.count_nonzero(agent.Q)}\")\nprint(f\"Q-table min/max: {agent.Q.min():.2f} / {agent.Q.max():.2f}\")\n\nvisited_states = np.where(np.any(agent.Q != 0, axis=1))[0]\nprint(f\"States visited: {len(visited_states)} out of {agent.Q.shape[0]}\")",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
