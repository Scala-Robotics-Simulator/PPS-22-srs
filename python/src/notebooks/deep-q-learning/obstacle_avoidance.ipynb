{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99fe343d-531d-4322-a2e8-f022463c6e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da0e049-72bc-4aa9-b439-99d6733bc02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c2b7b4-9311-408d-971a-b767acf69bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 10:46:09.539188: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-31 10:46:09.564645: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-31 10:46:12.497944: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from training.dqnetwork import DQNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c457d270-5124-4734-b1d1-73977f914d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.deepqlearning.obstacle_avoidance_env import ObstacleAvoidanceEnv\n",
    "from utils.reader import get_yaml_path, read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d83e8069-b40a-4eac-895f-6111fbdf243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-31 10:46:13,390 — INFO — ✓ Connected to localhost:50051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "server_address = \"localhost:50051\"\n",
    "client_name = \"RLClient\"\n",
    "env = ObstacleAvoidanceEnv(server_address, client_name)\n",
    "env.connect_to_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce9acdd-5581-4205-843e-07704dcc556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = get_yaml_path(\"resources\", \"configurations\", \"obstacle-avoidance.yml\")\n",
    "config = read_file(config_path)\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8416fb-8844-48d2-9d3f-40dcd6a73fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-31 10:46:13,402 — INFO — ✓ Initialization successful\n"
     ]
    }
   ],
   "source": [
    "env.init(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38a6de7-d72e-436f-889c-e0782c5a0250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">825</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │           \u001b[38;5;34m825\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,481</span> (13.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,481\u001b[0m (13.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,481</span> (13.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,481\u001b[0m (13.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">825</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │           \u001b[38;5;34m825\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,481</span> (13.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,481\u001b[0m (13.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,481</span> (13.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,481\u001b[0m (13.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "neuron_count_per_hidden_layer = [64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73702127-b7a9-49db-9013-35f5eb268d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_count = 500  # Total number of training episodes\n",
    "episode_max_steps = 5000  # Maximum number of steps per episode\n",
    "\n",
    "replay_memory_max_size = (\n",
    "    100000  # Maximum number of transitions stored into the replay memory\n",
    ")\n",
    "replay_memory_init_size = (\n",
    "    1000  # Maximum number of transitions stored into the replay memory\n",
    ")\n",
    "batch_size = 64  # Mini-batch size\n",
    "\n",
    "step_per_update = 4  # Number of total steps executed between successive updates of the action model weights\n",
    "step_per_update_target_model = 8  # Number of total steps executed between successive replaces of the target model weights\n",
    "\n",
    "max_epsilon = 1.0  # Exploration probability at start\n",
    "min_epsilon = 0.01  # Minimum exploration probability\n",
    "epsilon_decay = 0.0002  # Decay for exploration probability\n",
    "\n",
    "gamma = 0.99  # Discount factor\n",
    "\n",
    "moving_avg_window_size = 20  # Number of consecutive episodes to be considered in the calculation of the total reward moving average\n",
    "moving_avg_stop_thr = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca508643-118b-482d-9772-2787d3b94515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n",
      "/nix/store/xfj6pw68yzxq174as0lggbfpqkgxn6z5-python3-3.13.8-env/lib/python3.13/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "from agent.scala_dqagent import DQAgent\n",
    "\n",
    "agent1 = DQAgent(\n",
    "    env,\n",
    "    agent_id=\"00000000-0000-0000-0000-000000000001\",\n",
    "    action_model=DQNetwork(\n",
    "        env.observation_space.shape,\n",
    "        neuron_count_per_hidden_layer,\n",
    "        env.action_space.n,\n",
    "        summary=False,\n",
    "    ),\n",
    "    target_model=DQNetwork(\n",
    "        env.observation_space.shape,\n",
    "        neuron_count_per_hidden_layer,\n",
    "        env.action_space.n,\n",
    "        summary=False,\n",
    "    ),\n",
    "    epsilon_max=max_epsilon,\n",
    "    epsilon_min=min_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    gamma=gamma,\n",
    "    replay_memory_max_size=replay_memory_max_size,\n",
    "    replay_memory_init_size=replay_memory_init_size,\n",
    "    batch_size=batch_size,\n",
    "    step_per_update=step_per_update,\n",
    "    step_per_update_target_model=step_per_update_target_model,\n",
    "    moving_avg_window_size=moving_avg_window_size,\n",
    "    moving_avg_stop_thr=moving_avg_stop_thr,\n",
    "    episode_max_steps=episode_max_steps,\n",
    ")\n",
    "\n",
    "agents = [agent1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ddf5e41-468e-4a66-b289-af185a08f3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DQN:   0%|                                                                                                                                                                                                     | 0/500 [00:00<?, ?ep/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-31 10:48:09,387 — INFO — Episode: 0 | Steps: 2774[2774] | Epsilon (of the first agent): 1.000 | Time: 108.72s | Reward (of the first agent): -96060.7 | MovingAvg (of the first agent): -96060.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DQN:   0%|▎                                                                                                                                                                                        | 1/500 [01:48<15:04:11, 108.72s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-31 10:48:54,917 — INFO — Episode: 1 | Steps: 1004[3778] | Epsilon (of the first agent): 0.445 | Time: 45.53s | Reward (of the first agent): 160.0 | MovingAvg (of the first agent): 160.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DQN:   0%|▋                                                                                                                                                                                          | 2/500 [02:34<9:53:51, 71.55s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-31 10:50:34,860 — INFO — Episode: 2 | Steps: 1505[5283] | Epsilon (of the first agent): 0.362 | Time: 99.94s | Reward (of the first agent): -90041.6 | MovingAvg (of the first agent): -90041.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training DQN:   1%|█                                                                                                                                                                                         | 3/500 [04:29<12:23:18, 89.73s/ep]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      5\u001b[39m train_start_time = time.time()\n\u001b[32m      7\u001b[39m trainer = DQLearning(\n\u001b[32m      8\u001b[39m     env,\n\u001b[32m      9\u001b[39m     agents,\n\u001b[32m     10\u001b[39m     episode_count=episode_count,\n\u001b[32m     11\u001b[39m     episode_max_steps=episode_max_steps,\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m train_rewards = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimple_dqn_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m train_finish_time = time.time()\n\u001b[32m     16\u001b[39m train_elapsed_time = train_finish_time - train_start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni-lab/PPS/2024/SRS/PPS-22-srs/feature/deepqlearning-obstacle-avoidance/python/src/notebooks/deep-q-learning/../../training/multi_agent_dqlearning.py:59\u001b[39m, in \u001b[36mDQLearning.simple_dqn_training\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     agent.terminated = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m step_count < \u001b[38;5;28mself\u001b[39m.episode_max_steps \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m     58\u001b[39m     actions = {\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m         agent.id: \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agents\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m agent.terminated\n\u001b[32m     62\u001b[39m     }\n\u001b[32m     64\u001b[39m     next_states, rewards, terminateds, truncateds, _ = \u001b[38;5;28mself\u001b[39m.env.step(\n\u001b[32m     65\u001b[39m         actions\n\u001b[32m     66\u001b[39m     )\n\u001b[32m     67\u001b[39m     dones = {\n\u001b[32m     68\u001b[39m         agent.id: terminateds[agent.id] \u001b[38;5;129;01mor\u001b[39;00m truncateds[agent.id]\n\u001b[32m     69\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agents\n\u001b[32m     70\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni-lab/PPS/2024/SRS/PPS-22-srs/feature/deepqlearning-obstacle-avoidance/python/src/notebooks/deep-q-learning/../../agent/scala_dqagent.py:116\u001b[39m, in \u001b[36mDQAgent.choose_action\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m random.uniform(\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m) <= \u001b[38;5;28mself\u001b[39m.epsilon:\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.env.action_space.sample()\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m q_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maction_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.argmax(q_values)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/xfj6pw68yzxq174as0lggbfpqkgxn6z5-python3-3.13.8-env/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/xfj6pw68yzxq174as0lggbfpqkgxn6z5-python3-3.13.8-env/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:505\u001b[39m, in \u001b[36mTensorFlowTrainer.predict\u001b[39m\u001b[34m(self, x, batch_size, verbose, steps, callbacks)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;129m@traceback_utils\u001b[39m.filter_traceback\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m    502\u001b[39m     \u001b[38;5;28mself\u001b[39m, x, batch_size=\u001b[38;5;28;01mNone\u001b[39;00m, verbose=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, steps=\u001b[38;5;28;01mNone\u001b[39;00m, callbacks=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    503\u001b[39m ):\n\u001b[32m    504\u001b[39m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     epoch_iterator = \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    514\u001b[39m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[32m    515\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module.CallbackList):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/xfj6pw68yzxq174as0lggbfpqkgxn6z5-python3-3.13.8-env/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:728\u001b[39m, in \u001b[36mTFEpochIterator.__init__\u001b[39m\u001b[34m(self, distribute_strategy, *args, **kwargs)\u001b[39m\n\u001b[32m    726\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(*args, **kwargs)\n\u001b[32m    727\u001b[39m \u001b[38;5;28mself\u001b[39m._distribute_strategy = distribute_strategy\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_adapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf.distribute.DistributedDataset):\n\u001b[32m    730\u001b[39m     dataset = \u001b[38;5;28mself\u001b[39m._distribute_strategy.experimental_distribute_dataset(\n\u001b[32m    731\u001b[39m         dataset\n\u001b[32m    732\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/xfj6pw68yzxq174as0lggbfpqkgxn6z5-python3-3.13.8-env/lib/python3.13/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:235\u001b[39m, in \u001b[36mArrayDataAdapter.get_tf_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    233\u001b[39m     indices_dataset = indices_dataset.map(tf.random.shuffle)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m dataset = \u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m options = tf.data.Options()\n\u001b[32m    238\u001b[39m options.experimental_distribute.auto_shard_policy = (\n\u001b[32m    239\u001b[39m     tf.data.experimental.AutoShardPolicy.DATA\n\u001b[32m    240\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/xfj6pw68yzxq174as0lggbfpqkgxn6z5-python3-3.13.8-env/lib/python3.13/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:228\u001b[39m, in \u001b[36mArrayDataAdapter.get_tf_dataset.<locals>.slice_inputs\u001b[39m\u001b[34m(indices_dataset, inputs)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shuffle:\n\u001b[32m    225\u001b[39m     options.experimental_external_state_policy = (\n\u001b[32m    226\u001b[39m         tf.data.experimental.ExternalStatePolicy.IGNORE\n\u001b[32m    227\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/xfj6pw68yzxq174as0lggbfpqkgxn6z5-python3-3.13.8-env/lib/python3.13/site-packages/tensorflow/python/data/ops/dataset_ops.py:3012\u001b[39m, in \u001b[36mDatasetV2.with_options\u001b[39m\u001b[34m(self, options, name)\u001b[39m\n\u001b[32m   2986\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_options\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, name=\u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[33m\"\u001b[39m\u001b[33mDatasetV2\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2987\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns a new `tf.data.Dataset` with the given options set.\u001b[39;00m\n\u001b[32m   2988\u001b[39m \n\u001b[32m   2989\u001b[39m \u001b[33;03m  The options are \"global\" in the sense they apply to the entire dataset.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3010\u001b[39m \u001b[33;03m    ValueError: when an option is set more than once to a non-default value\u001b[39;00m\n\u001b[32m   3011\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3012\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_OptionsDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/xfj6pw68yzxq174as0lggbfpqkgxn6z5-python3-3.13.8-env/lib/python3.13/site-packages/tensorflow/python/data/ops/dataset_ops.py:4909\u001b[39m, in \u001b[36m_OptionsDataset.__init__\u001b[39m\u001b[34m(self, input_dataset, options, name)\u001b[39m\n\u001b[32m   4907\u001b[39m \u001b[38;5;28mself\u001b[39m._name = name\n\u001b[32m   4908\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(input_dataset._variant_tensor):\n\u001b[32m-> \u001b[39m\u001b[32m4909\u001b[39m   variant_tensor = \u001b[43mgen_dataset_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptions_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4910\u001b[39m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_pb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4911\u001b[39m \u001b[43m      \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_common_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4912\u001b[39m \u001b[38;5;28msuper\u001b[39m(_OptionsDataset, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m(input_dataset, variant_tensor)\n\u001b[32m   4914\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._options_attr:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/xfj6pw68yzxq174as0lggbfpqkgxn6z5-python3-3.13.8-env/lib/python3.13/site-packages/tensorflow/python/ops/gen_dataset_ops.py:4663\u001b[39m, in \u001b[36moptions_dataset\u001b[39m\u001b[34m(input_dataset, serialized_options, output_types, output_shapes, metadata, name)\u001b[39m\n\u001b[32m   4661\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m   4662\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4663\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4664\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOptionsDataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mserialized_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4665\u001b[39m \u001b[43m      \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_types\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_shapes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4666\u001b[39m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4667\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   4668\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from training.multi_agent_dqlearning import DQLearning\n",
    "\n",
    "train_start_time = time.time()\n",
    "\n",
    "trainer = DQLearning(\n",
    "    env,\n",
    "    agents,\n",
    "    episode_count=episode_count,\n",
    "    episode_max_steps=episode_max_steps,\n",
    ")\n",
    "train_rewards = trainer.simple_dqn_training()\n",
    "\n",
    "train_finish_time = time.time()\n",
    "train_elapsed_time = train_finish_time - train_start_time\n",
    "train_avg_episode_time = train_elapsed_time / episode_count\n",
    "\n",
    "print(\n",
    "    f\"Train time: {train_elapsed_time / 60.0:.1f}m [{train_avg_episode_time:.1f}s]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80c6e9b1-9016-432e-92b9-b8016803eff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-31 10:51:13,195 — INFO — Episode 1/5 - Reward: 78.58215596839916\n",
      "2025-10-31 10:51:13,198 — INFO — Episode 2/5 - Reward: 0\n",
      "2025-10-31 10:51:13,198 — INFO — Episode 3/5 - Reward: 0\n",
      "2025-10-31 10:51:13,199 — INFO — Episode 4/5 - Reward: 0\n",
      "2025-10-31 10:51:13,200 — INFO — Episode 5/5 - Reward: 0\n",
      "2025-10-31 10:51:13,200 — INFO — ✓ Closed connection to localhost:50051\n"
     ]
    }
   ],
   "source": [
    "trainer.play_with_pygame(episodes=5, fps=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
