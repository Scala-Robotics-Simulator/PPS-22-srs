{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99fe343d-531d-4322-a2e8-f022463c6e13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da0e049-72bc-4aa9-b439-99d6733bc02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 16:21:07.068869: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c2b7b4-9311-408d-971a-b767acf69bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.dqnetwork import DQNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c457d270-5124-4734-b1d1-73977f914d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.deepqlearning.obstacle_avoidance_env import ObstacleAvoidanceEnv\n",
    "from utils.reader import get_yaml_path, read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d83e8069-b40a-4eac-895f-6111fbdf243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-01 16:21:13,487 — INFO — ✓ Connected to localhost:50060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "server_address = \"localhost:50060\"\n",
    "client_name = \"RLClient\"\n",
    "env = ObstacleAvoidanceEnv(server_address, client_name)\n",
    "env.connect_to_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce9acdd-5581-4205-843e-07704dcc556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = get_yaml_path(\"resources\", \"configurations\", \"obstacle-avoidance.yml\")\n",
    "config = read_file(config_path)\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8416fb-8844-48d2-9d3f-40dcd6a73fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-01 16:21:13,513 — INFO — ✓ Initialization successful\n"
     ]
    }
   ],
   "source": [
    "env.init(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38a6de7-d72e-436f-889c-e0782c5a0250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neuron_count_per_hidden_layer = [64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73702127-b7a9-49db-9013-35f5eb268d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_count = 1000  # Total number of training episodes\n",
    "episode_max_steps = 2000  # Maximum number of steps per episode\n",
    "\n",
    "replay_memory_max_size = (\n",
    "    100000  # Maximum number of transitions stored into the replay memory\n",
    ")\n",
    "replay_memory_init_size = (\n",
    "    10000  # Maximum number of transitions stored into the replay memory\n",
    ")\n",
    "batch_size = 512  # Mini-batch size\n",
    "\n",
    "step_per_update = 4  # Number of total steps executed between successive updates of the action model weights\n",
    "step_per_update_target_model = 1000  # Number of total steps executed between successive replaces of the target model weights\n",
    "\n",
    "max_epsilon = 1.0  # Exploration probability at start\n",
    "min_epsilon = 0.01  # Minimum exploration probability\n",
    "epsilon_decay = 0.0002  # Decay for exploration probability\n",
    "\n",
    "gamma = 0.99  # Discount factor\n",
    "\n",
    "moving_avg_window_size = 20  # Number of consecutive episodes to be considered in the calculation of the total reward moving average\n",
    "moving_avg_stop_thr = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca508643-118b-482d-9772-2787d3b94515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1762010473.607651   13969 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4081 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:1d:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">825</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m576\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │           \u001b[38;5;34m825\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,481</span> (13.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,481\u001b[0m (13.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,481</span> (13.60 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,481\u001b[0m (13.60 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01magent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscala_dqagent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DQAgent\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m agent1 = \u001b[43mDQAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43magent_id\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m00000000-0000-0000-0000-000000000001\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43maction_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDQNetwork\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneuron_count_per_hidden_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43msummary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDQNetwork\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mneuron_count_per_hidden_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43msummary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepsilon_max\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_epsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepsilon_min\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmin_epsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreplay_memory_max_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreplay_memory_max_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreplay_memory_init_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreplay_memory_init_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep_per_update\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_per_update\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mupdates_per_training\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdates_per_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep_per_update_target_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_per_update_target_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmoving_avg_window_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmoving_avg_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmoving_avg_stop_thr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmoving_avg_stop_thr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepisode_max_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepisode_max_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepisode_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m agents = [agent1]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/PPS-22-srs/python/src/notebooks/deep-q-learning/../../agent/scala_dqagent.py:105\u001b[39m, in \u001b[36mDQAgent.__init__\u001b[39m\u001b[34m(self, env, agent_id, action_model, target_model, epsilon_max, epsilon_min, gamma, replay_memory_max_size, replay_memory_init_size, batch_size, step_per_update, updates_per_training, step_per_update_target_model, moving_avg_window_size, moving_avg_stop_thr, episode_max_steps, episodes)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28mself\u001b[39m._create_tf_functions()\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m replay_memory_init_size > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimple_dqn_replay_memory_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplay_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_memory_init_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_max_steps\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/PPS-22-srs/python/src/notebooks/deep-q-learning/../../agent/scala_dqagent.py:245\u001b[39m, in \u001b[36mDQAgent.simple_dqn_replay_memory_init\u001b[39m\u001b[34m(self, env, replay_memory, replay_memory_init_size, episode_max_steps)\u001b[39m\n\u001b[32m    243\u001b[39m action = env.action_space.sample()\n\u001b[32m    244\u001b[39m actions = {\u001b[38;5;28mself\u001b[39m.id: action}\n\u001b[32m--> \u001b[39m\u001b[32m245\u001b[39m new_states, rewards, terminateds, truncateds, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    246\u001b[39m done = terminateds[\u001b[38;5;28mself\u001b[39m.id] \u001b[38;5;129;01mor\u001b[39;00m truncateds[\u001b[38;5;28mself\u001b[39m.id]\n\u001b[32m    247\u001b[39m new_state = new_states[\u001b[38;5;28mself\u001b[39m.id]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/PPS-22-srs/python/src/notebooks/deep-q-learning/../../environment/abstract_env.py:103\u001b[39m, in \u001b[36mAbstractEnv.step\u001b[39m\u001b[34m(self, actions)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Take a step in the environment with the given actions\u001b[39;00m\n\u001b[32m     90\u001b[39m \n\u001b[32m     91\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     99\u001b[39m \u001b[33;03m    A tuple containing observations, rewards, terminateds, truncateds, and infos.\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    101\u001b[39m actions = \u001b[38;5;28mself\u001b[39m._decode_actions(actions)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m observations, rewards, terminateds, truncateds, infos = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_async\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    107\u001b[39m     \u001b[38;5;28mself\u001b[39m._encode_observations(observations),\n\u001b[32m    108\u001b[39m     rewards,\n\u001b[32m   (...)\u001b[39m\u001b[32m    111\u001b[39m     infos,\n\u001b[32m    112\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/PPS-22-srs/python/src/notebooks/deep-q-learning/../../environment/abstract_env.py:42\u001b[39m, in \u001b[36mAbstractEnv._run_async\u001b[39m\u001b[34m(self, coro)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, coro):\n\u001b[32m     41\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Helper method to run async coroutines synchronously\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/pwrhqv9fy9lbybjz9c1w283z234lzksr-python3-3.13.8-env/lib/python3.13/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/pwrhqv9fy9lbybjz9c1w283z234lzksr-python3-3.13.8-env/lib/python3.13/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/cfapjd2rvqrpry4grb0kljnp8bvnvfxz-python3-3.13.8/lib/python3.13/selectors.py:452\u001b[39m, in \u001b[36mEpollSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    450\u001b[39m ready = []\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from agent.scala_dqagent import DQAgent\n",
    "\n",
    "agent1 = DQAgent(\n",
    "    env,\n",
    "    agent_id=\"00000000-0000-0000-0000-000000000001\",\n",
    "    action_model=DQNetwork(\n",
    "        env.observation_space.shape,\n",
    "        neuron_count_per_hidden_layer,\n",
    "        env.action_space.n,\n",
    "        summary=True,\n",
    "    ),\n",
    "    target_model=DQNetwork(\n",
    "        env.observation_space.shape,\n",
    "        neuron_count_per_hidden_layer,\n",
    "        env.action_space.n,\n",
    "        summary=False,\n",
    "    ),\n",
    "    epsilon_max=max_epsilon,\n",
    "    epsilon_min=min_epsilon,\n",
    "    gamma=gamma,\n",
    "    replay_memory_max_size=replay_memory_max_size,\n",
    "    replay_memory_init_size=replay_memory_init_size,\n",
    "    batch_size=batch_size,\n",
    "    step_per_update=step_per_update,\n",
    "    step_per_update_target_model=step_per_update_target_model,\n",
    "    moving_avg_window_size=moving_avg_window_size,\n",
    "    moving_avg_stop_thr=moving_avg_stop_thr,\n",
    "    episode_max_steps=episode_max_steps,\n",
    "    episodes=episode_count,\n",
    ")\n",
    "\n",
    "agents = [agent1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf5e41-468e-4a66-b289-af185a08f3a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from training.multi_agent_dqlearning import DQLearning\n",
    "\n",
    "train_start_time = time.time()\n",
    "\n",
    "trainer = DQLearning(\n",
    "    env,\n",
    "    agents,\n",
    "    episode_count=episode_count,\n",
    "    episode_max_steps=episode_max_steps,\n",
    ")\n",
    "train_rewards = trainer.simple_dqn_training()\n",
    "\n",
    "train_finish_time = time.time()\n",
    "train_elapsed_time = train_finish_time - train_start_time\n",
    "train_avg_episode_time = train_elapsed_time / episode_count\n",
    "\n",
    "print(\n",
    "    f\"Train time: {train_elapsed_time / 60.0:.1f}m [{train_avg_episode_time:.1f}s]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6e9b1-9016-432e-92b9-b8016803eff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.play_with_pygame(episodes=5, fps=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
