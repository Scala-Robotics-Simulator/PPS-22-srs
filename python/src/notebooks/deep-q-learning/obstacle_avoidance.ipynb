{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99fe343d-531d-4322-a2e8-f022463c6e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da0e049-72bc-4aa9-b439-99d6733bc02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c2b7b4-9311-408d-971a-b767acf69bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 15:21:19.383130: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-31 15:21:19.407087: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-31 15:21:22.282728: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from training.dqnetwork import DQNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c457d270-5124-4734-b1d1-73977f914d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.deepqlearning.obstacle_avoidance_env import ObstacleAvoidanceEnv\n",
    "from utils.reader import get_yaml_path, read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d83e8069-b40a-4eac-895f-6111fbdf243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-31 15:21:23,187 — INFO — ✓ Connected to localhost:50051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "server_address = \"localhost:50051\"\n",
    "client_name = \"RLClient\"\n",
    "env = ObstacleAvoidanceEnv(server_address, client_name)\n",
    "env.connect_to_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce9acdd-5581-4205-843e-07704dcc556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = get_yaml_path(\"resources\", \"configurations\", \"obstacle-avoidance.yml\")\n",
    "config = read_file(config_path)\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8416fb-8844-48d2-9d3f-40dcd6a73fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-31 15:21:23,203 — INFO — ✓ Initialization successful\n"
     ]
    }
   ],
   "source": [
    "env.init(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38a6de7-d72e-436f-889c-e0782c5a0250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neuron_count_per_hidden_layer = [64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73702127-b7a9-49db-9013-35f5eb268d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_count = 50  # Total number of training episodes\n",
    "episode_max_steps = 2000  # Maximum number of steps per episode\n",
    "\n",
    "replay_memory_max_size = (\n",
    "    100000  # Maximum number of transitions stored into the replay memory\n",
    ")\n",
    "replay_memory_init_size = (\n",
    "    1000  # Maximum number of transitions stored into the replay memory\n",
    ")\n",
    "batch_size = 64  # Mini-batch size\n",
    "\n",
    "step_per_update = 4  # Number of total steps executed between successive updates of the action model weights\n",
    "step_per_update_target_model = 8  # Number of total steps executed between successive replaces of the target model weights\n",
    "\n",
    "max_epsilon = 1.0  # Exploration probability at start\n",
    "min_epsilon = 0.01  # Minimum exploration probability\n",
    "epsilon_decay = 0.0002  # Decay for exploration probability\n",
    "\n",
    "gamma = 0.99  # Discount factor\n",
    "\n",
    "moving_avg_window_size = 20  # Number of consecutive episodes to be considered in the calculation of the total reward moving average\n",
    "moving_avg_stop_thr = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca508643-118b-482d-9772-2787d3b94515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.scala_dqagent import DQAgent\n",
    "\n",
    "agent1 = DQAgent(\n",
    "    env,\n",
    "    agent_id=\"00000000-0000-0000-0000-000000000001\",\n",
    "    action_model=DQNetwork(\n",
    "        env.observation_space.shape,\n",
    "        neuron_count_per_hidden_layer,\n",
    "        env.action_space.n,\n",
    "        summary=False,\n",
    "    ),\n",
    "    target_model=DQNetwork(\n",
    "        env.observation_space.shape,\n",
    "        neuron_count_per_hidden_layer,\n",
    "        env.action_space.n,\n",
    "        summary=False,\n",
    "    ),\n",
    "    epsilon_max=max_epsilon,\n",
    "    epsilon_min=min_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    gamma=gamma,\n",
    "    replay_memory_max_size=replay_memory_max_size,\n",
    "    replay_memory_init_size=replay_memory_init_size,\n",
    "    batch_size=batch_size,\n",
    "    step_per_update=step_per_update,\n",
    "    step_per_update_target_model=step_per_update_target_model,\n",
    "    moving_avg_window_size=moving_avg_window_size,\n",
    "    moving_avg_stop_thr=moving_avg_stop_thr,\n",
    "    episode_max_steps=episode_max_steps,\n",
    ")\n",
    "\n",
    "agents = [agent1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ddf5e41-468e-4a66-b289-af185a08f3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:488: RuntimeWarning: Your system is avx2 capable but pygame was not built with support for it. The performance of some of your blits could be adversely affected. Consider enabling compile time detection with environment variables like PYGAME_DETECT_AVX2=1 if you are compiling without cross compilation.\n",
      "/nix/store/xfj6pw68yzxq174as0lggbfpqkgxn6z5-python3-3.13.8-env/lib/python3.13/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "Training DQN:   0%|                                                                                                                                                                                                      | 0/50 [00:45<?, ?ep/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to dict.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      5\u001b[39m train_start_time = time.time()\n\u001b[32m      7\u001b[39m trainer = DQLearning(\n\u001b[32m      8\u001b[39m     env,\n\u001b[32m      9\u001b[39m     agents,\n\u001b[32m     10\u001b[39m     episode_count=episode_count,\n\u001b[32m     11\u001b[39m     episode_max_steps=episode_max_steps,\n\u001b[32m     12\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m train_rewards = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimple_dqn_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m train_finish_time = time.time()\n\u001b[32m     16\u001b[39m train_elapsed_time = train_finish_time - train_start_time\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uni-lab/PPS/2024/SRS/PPS-22-srs/feature/deepqlearning-obstacle-avoidance/python/src/notebooks/deep-q-learning/../../training/multi_agent_dqlearning.py:120\u001b[39m, in \u001b[36mDQLearning.simple_dqn_training\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    107\u001b[39m     moving_avg_reward = {\n\u001b[32m    108\u001b[39m         agent.id: (\n\u001b[32m    109\u001b[39m             statistics.mean(train_rewards[-agent.moving_avg_window_size :])\n\u001b[32m   (...)\u001b[39m\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agents\n\u001b[32m    114\u001b[39m     }\n\u001b[32m    115\u001b[39m     train_rewards.append(episode_reward)\n\u001b[32m    117\u001b[39m     logger.info(\n\u001b[32m    118\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpisode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_step_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpsilon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_epsilon\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | MovingAvg (of the first agent): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmoving_avg_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    121\u001b[39m     )\n\u001b[32m    123\u001b[39m     \u001b[38;5;66;03m# if (\u001b[39;00m\n\u001b[32m    124\u001b[39m     \u001b[38;5;66;03m#     self.agent.moving_avg_stop_thr\u001b[39;00m\n\u001b[32m    125\u001b[39m     \u001b[38;5;66;03m#     and moving_avg_reward >= self.agent.moving_avg_stop_thr\u001b[39;00m\n\u001b[32m    126\u001b[39m     \u001b[38;5;66;03m# ):\u001b[39;00m\n\u001b[32m    127\u001b[39m     \u001b[38;5;66;03m#     break\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m train_rewards\n",
      "\u001b[31mTypeError\u001b[39m: unsupported format string passed to dict.__format__"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from training.multi_agent_dqlearning import DQLearning\n",
    "\n",
    "train_start_time = time.time()\n",
    "\n",
    "trainer = DQLearning(\n",
    "    env,\n",
    "    agents,\n",
    "    episode_count=episode_count,\n",
    "    episode_max_steps=episode_max_steps,\n",
    ")\n",
    "train_rewards = trainer.simple_dqn_training()\n",
    "\n",
    "train_finish_time = time.time()\n",
    "train_elapsed_time = train_finish_time - train_start_time\n",
    "train_avg_episode_time = train_elapsed_time / episode_count\n",
    "\n",
    "print(\n",
    "    f\"Train time: {train_elapsed_time / 60.0:.1f}m [{train_avg_episode_time:.1f}s]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6e9b1-9016-432e-92b9-b8016803eff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.play_with_pygame(episodes=5, fps=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
