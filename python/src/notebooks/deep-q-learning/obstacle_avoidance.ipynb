{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99fe343d-531d-4322-a2e8-f022463c6e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da0e049-72bc-4aa9-b439-99d6733bc02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6c2b7b4-9311-408d-971a-b767acf69bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 16:39:59.426296: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-31 16:39:59.454423: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-31 16:40:02.330892: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from training.dqnetwork import DQNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c457d270-5124-4734-b1d1-73977f914d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.deepqlearning.obstacle_avoidance_env import ObstacleAvoidanceEnv\n",
    "from utils.reader import get_yaml_path, read_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d83e8069-b40a-4eac-895f-6111fbdf243a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-31 16:40:03,228 — INFO — ✓ Connected to localhost:50051\n",
      "\n"
     ]
    }
   ],
   "source": [
    "server_address = \"localhost:50051\"\n",
    "client_name = \"RLClient\"\n",
    "env = ObstacleAvoidanceEnv(server_address, client_name)\n",
    "env.connect_to_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ce9acdd-5581-4205-843e-07704dcc556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = get_yaml_path(\"resources\", \"configurations\", \"obstacle-avoidance.yml\")\n",
    "config = read_file(config_path)\n",
    "# print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8416fb-8844-48d2-9d3f-40dcd6a73fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-31 16:40:03,240 — INFO — ✓ Initialization successful\n"
     ]
    }
   ],
   "source": [
    "env.init(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38a6de7-d72e-436f-889c-e0782c5a0250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "neuron_count_per_hidden_layer = [64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73702127-b7a9-49db-9013-35f5eb268d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_count = 50  # Total number of training episodes\n",
    "episode_max_steps = 2000  # Maximum number of steps per episode\n",
    "\n",
    "replay_memory_max_size = (\n",
    "    100000  # Maximum number of transitions stored into the replay memory\n",
    ")\n",
    "replay_memory_init_size = (\n",
    "    1000  # Maximum number of transitions stored into the replay memory\n",
    ")\n",
    "batch_size = 64  # Mini-batch size\n",
    "\n",
    "step_per_update = 4  # Number of total steps executed between successive updates of the action model weights\n",
    "step_per_update_target_model = 8  # Number of total steps executed between successive replaces of the target model weights\n",
    "\n",
    "max_epsilon = 1.0  # Exploration probability at start\n",
    "min_epsilon = 0.01  # Minimum exploration probability\n",
    "epsilon_decay = 0.0002  # Decay for exploration probability\n",
    "\n",
    "gamma = 0.99  # Discount factor\n",
    "\n",
    "moving_avg_window_size = 20  # Number of consecutive episodes to be considered in the calculation of the total reward moving average\n",
    "moving_avg_stop_thr = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca508643-118b-482d-9772-2787d3b94515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.scala_dqagent import DQAgent\n",
    "\n",
    "agent1 = DQAgent(\n",
    "    env,\n",
    "    agent_id=\"00000000-0000-0000-0000-000000000001\",\n",
    "    action_model=DQNetwork(\n",
    "        env.observation_space.shape,\n",
    "        neuron_count_per_hidden_layer,\n",
    "        env.action_space.n,\n",
    "        summary=False,\n",
    "    ),\n",
    "    target_model=DQNetwork(\n",
    "        env.observation_space.shape,\n",
    "        neuron_count_per_hidden_layer,\n",
    "        env.action_space.n,\n",
    "        summary=False,\n",
    "    ),\n",
    "    epsilon_max=max_epsilon,\n",
    "    epsilon_min=min_epsilon,\n",
    "    gamma=gamma,\n",
    "    replay_memory_max_size=replay_memory_max_size,\n",
    "    replay_memory_init_size=replay_memory_init_size,\n",
    "    batch_size=batch_size,\n",
    "    step_per_update=step_per_update,\n",
    "    step_per_update_target_model=step_per_update_target_model,\n",
    "    moving_avg_window_size=moving_avg_window_size,\n",
    "    moving_avg_stop_thr=moving_avg_stop_thr,\n",
    "    episode_max_steps=episode_max_steps,\n",
    "    episodes=episode_count,\n",
    ")\n",
    "\n",
    "agents = [agent1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf5e41-468e-4a66-b289-af185a08f3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from training.multi_agent_dqlearning import DQLearning\n",
    "\n",
    "train_start_time = time.time()\n",
    "\n",
    "trainer = DQLearning(\n",
    "    env,\n",
    "    agents,\n",
    "    episode_count=episode_count,\n",
    "    episode_max_steps=episode_max_steps,\n",
    ")\n",
    "train_rewards = trainer.simple_dqn_training()\n",
    "\n",
    "train_finish_time = time.time()\n",
    "train_elapsed_time = train_finish_time - train_start_time\n",
    "train_avg_episode_time = train_elapsed_time / episode_count\n",
    "\n",
    "print(\n",
    "    f\"Train time: {train_elapsed_time / 60.0:.1f}m [{train_avg_episode_time:.1f}s]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c6e9b1-9016-432e-92b9-b8016803eff6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.play_with_pygame(episodes=5, fps=60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
